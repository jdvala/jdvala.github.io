<!doctype html><html lang=en><head><meta charset=utf-8><title>Jay Vala</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A picture is worth thousand words"><meta property="og:title" content="Tensorboard"><meta property="og:description" content="A picture is worth thousand words"><meta property="og:type" content="website"><meta property="og:url" content="https://jdvala.github.io/posts/2018-10-23-visualizing-model-using-tensorboard/"><meta itemprop=name content="Tensorboard"><meta itemprop=description content="A picture is worth thousand words"><meta name=twitter:card content="summary"><meta name=twitter:title content="Tensorboard"><meta name=twitter:description content="A picture is worth thousand words"><link rel=apple-touch-icon sizes=180x180 href=apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=favicon-32.png><link rel=stylesheet href=https://jdvala.github.io/scss/style.min.d1aa507e320f63a9a89fb4d16c025955cea1564900de1060a4b2d7cabbabcdec.css></head><body><header><div class="header header-frame"><div><h1 class=header__title>Tensorboard</h1><div class=header__description>A picture is worth thousand words</div></div><nav class=header-nav><ul class="header-nav-list header-nav-list--menu"><li class=header-nav-list__item><a class=header-nav-list__link href=/about/><span>About</span></a></li></ul><button class=header-nav-list__nav-btn>navigation</button></nav><button class=mb-header__menu-btn>
<span class=mb-header__menu-btn-line></span><span class=mb-header__menu-btn-line></span><span class=mb-header__menu-btn-line></span></button></div><nav id=mobile-header-nav class=mb-header-nav><button class="mb-header-nav__close-btn flex-center"><svg class="mb-header-nav__svg-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="32" height="32"
            ><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"
                /><path d="M0 0h24v24H0z" fill="none" /></svg></button><div class=mb-header-nav__wrapper><div class=mb-header-nav__container><svg width="240" height="72" viewBox="0 0 240 72" class="mb-header-nav__title"
                ><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle">Tags</text></svg><ul class=mb-header-nav-list><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/python/>python</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/text/>text</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/nlp/>nlp</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/analysis/>analysis</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/ai/>AI</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/tensorflow/>tensorflow</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/visualization/>visualization</a></li></ul></div><div class=mb-header-nav__container><svg width="240" height="72" viewBox="0 0 240 72" class="mb-header-nav__title"
                ><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle">Menu</text></svg><ul class=mb-header-nav-list><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=/about/>About</a></li></ul></div></div></nav></header><div id=content><article class=post><div class=post-content><p>In this assignment we are going to use <em>Tensorboard</em> a tool provided with tensorflow to visualize and debug(if necessary) our neural networks visually. I am going to visualize all the parameteres that have the properties which enables us to debug our neural networks or gives us a fair intution on how and what goes wrong.</p><p>I will try and do this assignment in google colab, although it will be difficult, I will try my best.</p><p>For using tensorboard on google colab we need something called we just need to install a single library using pip.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a61717;background-color:#e3d2d2>!</span>pip install tensorboardcolab
</code></pre></div><pre><code>Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.19)
</code></pre><p>Now that we have everything setup and running we have to train a model to get some output on tensorboard</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#000;font-weight:700>import</span> <span style=color:#555>tensorflow</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>tf</span>

<span style=color:#000;font-weight:700>from</span> <span style=color:#555>tensorboardcolab</span> <span style=color:#000;font-weight:700>import</span> TensorBoardColab

</code></pre></div><pre><code>Using TensorFlow backend.
</code></pre><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># Getting MNIST</span>
<span style=color:#998;font-style:italic># uploading the data</span>
<span style=color:#000;font-weight:700>from</span> <span style=color:#555>google.colab</span> <span style=color:#000;font-weight:700>import</span> files
files<span style=color:#000;font-weight:700>.</span>upload()
</code></pre></div><p>Now that the data is loaded, I will create train set and test set from these files and then convert the labels into <em>One-hot encoded ones</em></p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#000;font-weight:700>import</span> <span style=color:#555>pandas</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>pd</span>
train_df <span style=color:#000;font-weight:700>=</span> pd<span style=color:#000;font-weight:700>.</span>read_csv(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>mnist_train.csv</span><span style=color:#d14>&#39;</span>)
test_df <span style=color:#000;font-weight:700>=</span> pd<span style=color:#000;font-weight:700>.</span>read_csv(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>mnist_test.csv</span><span style=color:#d14>&#39;</span>)
</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># Seperating labels from the training and testing data</span>
train <span style=color:#000;font-weight:700>=</span> train_df<span style=color:#000;font-weight:700>.</span>values
trainSet <span style=color:#000;font-weight:700>=</span> train[:,<span style=color:#099>1</span>:]
trainLabel <span style=color:#000;font-weight:700>=</span> train[:,:<span style=color:#099>1</span>]
test <span style=color:#000;font-weight:700>=</span> test_df<span style=color:#000;font-weight:700>.</span>values
testSet <span style=color:#000;font-weight:700>=</span> test[:,<span style=color:#099>1</span>:]
testLabel <span style=color:#000;font-weight:700>=</span> test[:,:<span style=color:#099>1</span>]
</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># converting labels into one-hot-encoded values</span>
<span style=color:#000;font-weight:700>from</span> <span style=color:#555>sklearn.preprocessing</span> <span style=color:#000;font-weight:700>import</span> OneHotEncoder
le <span style=color:#000;font-weight:700>=</span> OneHotEncoder(handle_unknown<span style=color:#000;font-weight:700>=</span><span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>ignore</span><span style=color:#d14>&#39;</span>)

le<span style=color:#000;font-weight:700>.</span>fit(trainLabel<span style=color:#000;font-weight:700>.</span>reshape(<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>,<span style=color:#099>1</span>))
trainLabels <span style=color:#000;font-weight:700>=</span> le<span style=color:#000;font-weight:700>.</span>transform(trainLabel<span style=color:#000;font-weight:700>.</span>reshape(<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>,<span style=color:#099>1</span>))<span style=color:#000;font-weight:700>.</span>toarray()
testLabels <span style=color:#000;font-weight:700>=</span> le<span style=color:#000;font-weight:700>.</span>transform(testLabel<span style=color:#000;font-weight:700>.</span>reshape(<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>,<span style=color:#099>1</span>))<span style=color:#000;font-weight:700>.</span>toarray()
</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># Now defining the weights and baises. The input layer have no weight and no baise, </span>
<span style=color:#998;font-style:italic># and as I am creating a network with two hidden layer I will create weight and bais for hidden layer and output layer</span>
<span style=color:#998;font-style:italic># These weight are 2D arrays and baises are 1D arrays</span>

hidden_1_Weight <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>Variable(tf<span style=color:#000;font-weight:700>.</span>random_uniform([<span style=color:#099>784</span>, <span style=color:#099>256</span>]),name<span style=color:#000;font-weight:700>=</span><span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>weight_hidden_layer1</span><span style=color:#d14>&#39;</span>)  <span style=color:#998;font-style:italic># here we need to define what input this layer will be getting and what will be its output(This output is actually the number of hidden units this layer will have)</span>
hidden_2_Weight <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>Variable(tf<span style=color:#000;font-weight:700>.</span>random_uniform([<span style=color:#099>256</span>, <span style=color:#099>256</span>]), name<span style=color:#000;font-weight:700>=</span><span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>weight_hidden_layer2</span><span style=color:#d14>&#39;</span>)
outputWeight <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>Variable(tf<span style=color:#000;font-weight:700>.</span>random_uniform([<span style=color:#099>256</span>, <span style=color:#099>10</span>]), name<span style=color:#000;font-weight:700>=</span><span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>weight_output</span><span style=color:#d14>&#39;</span>)   <span style=color:#998;font-style:italic># here the input to this layer will be the output of previous layer i.e. 256 and its output will be number of classes it will predict i.e. 10</span>

<span style=color:#998;font-style:italic># biases</span>
hidden_1_Bias <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>Variable(tf<span style=color:#000;font-weight:700>.</span>random_uniform([<span style=color:#099>256</span>]), name<span style=color:#000;font-weight:700>=</span><span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>bais_hidden_layer1</span><span style=color:#d14>&#39;</span>)
hidden_2_Bias <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>Variable(tf<span style=color:#000;font-weight:700>.</span>random_uniform([<span style=color:#099>256</span>]), name<span style=color:#000;font-weight:700>=</span><span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>bais_hidden_layer2</span><span style=color:#d14>&#39;</span>)
outputBias <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>Variable(tf<span style=color:#000;font-weight:700>.</span>random_uniform([<span style=color:#099>10</span>]), name<span style=color:#000;font-weight:700>=</span><span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>bais_output</span><span style=color:#d14>&#39;</span>)
</code></pre></div><p>Now as the model weights and baises are completly defined, I will now add summaries to be displayed on tensorboard. So we have different summary options for different things, for example, we have <code>tf.summary.scalar</code> for displaying single values such as loss, accuracy and then we have <code>tf.summary.histogram</code> to visualize matries such as weights and baises . Also we can visualize layer activations and all other variables, but for the scope of this assignment I will only focus on a few of them.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># # Adding summaries</span>
hist_hidden_1 <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>summary<span style=color:#000;font-weight:700>.</span>histogram(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>hidden_1_weight</span><span style=color:#d14>&#39;</span>, hidden_1_Weight)
hist_hidden_2 <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>summary<span style=color:#000;font-weight:700>.</span>histogram(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>hidden_2_weight</span><span style=color:#d14>&#39;</span>, hidden_2_Weight)
output_layer_hist <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>summary<span style=color:#000;font-weight:700>.</span>histogram(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>Output_Weight</span><span style=color:#d14>&#39;</span>, outputWeight)
</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># PlaceHolders</span>
X <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>placeholder(tf<span style=color:#000;font-weight:700>.</span>float32, [<span style=color:#999>None</span>, <span style=color:#099>784</span>], name<span style=color:#000;font-weight:700>=</span><span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>input_placeholder</span><span style=color:#d14>&#39;</span>)
Y <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>placeholder(tf<span style=color:#000;font-weight:700>.</span>float32, [<span style=color:#999>None</span>, <span style=color:#099>10</span>], name<span style=color:#000;font-weight:700>=</span><span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>class_placeholder</span><span style=color:#d14>&#39;</span>)
</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># Lets define the function for multilayer perceptron</span>
<span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>perceptron</span>(x):
     <span style=color:#998;font-style:italic># Hidden fully connected layer with 128 neurons</span>
    layer_1 <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>nn<span style=color:#000;font-weight:700>.</span>relu(tf<span style=color:#000;font-weight:700>.</span>add(tf<span style=color:#000;font-weight:700>.</span>matmul(x, hidden_1_Weight), hidden_1_Bias))
    <span style=color:#998;font-style:italic># Hidden fully connected layer with 128 neurons</span>
    layer_2 <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>nn<span style=color:#000;font-weight:700>.</span>relu(tf<span style=color:#000;font-weight:700>.</span>add(tf<span style=color:#000;font-weight:700>.</span>matmul(layer_1, hidden_2_Weight), hidden_2_Bias))
    <span style=color:#998;font-style:italic># Output fully connected layer with a neuron for each class</span>
    out_layer <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>matmul(layer_2, outputWeight) <span style=color:#000;font-weight:700>+</span> outputBias
    
    <span style=color:#000;font-weight:700>return</span> out_layer

</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># build the network</span>
output <span style=color:#000;font-weight:700>=</span> perceptron(X)
</code></pre></div><p>For this assignment I am also going to calculate accuracy on each step or epoch just for visualization.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># Evaluate model</span>
correct_pred <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>equal(tf<span style=color:#000;font-weight:700>.</span>argmax(tf<span style=color:#000;font-weight:700>.</span>nn<span style=color:#000;font-weight:700>.</span>softmax(output), <span style=color:#099>1</span>), tf<span style=color:#000;font-weight:700>.</span>argmax(Y, <span style=color:#099>1</span>))
accuracy <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>reduce_mean(tf<span style=color:#000;font-weight:700>.</span>cast(correct_pred, tf<span style=color:#000;font-weight:700>.</span>float32))


<span style=color:#998;font-style:italic># adding accuracy scalar to the tensorboard</span>
accuracy_vis <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>summary<span style=color:#000;font-weight:700>.</span>scalar(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>accu</span><span style=color:#d14>&#39;</span>, accuracy)
</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># Define loss and optimizer</span>
loss <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>reduce_mean(tf<span style=color:#000;font-weight:700>.</span>nn<span style=color:#000;font-weight:700>.</span>softmax_cross_entropy_with_logits_v2(logits<span style=color:#000;font-weight:700>=</span>output, labels<span style=color:#000;font-weight:700>=</span>Y)) 
loss_vis <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>summary<span style=color:#000;font-weight:700>.</span>scalar(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>loss</span><span style=color:#d14>&#39;</span>, loss)
optimizer <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>train<span style=color:#000;font-weight:700>.</span>AdamOptimizer(learning_rate<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.01</span>) 
train_op <span style=color:#000;font-weight:700>=</span> optimizer<span style=color:#000;font-weight:700>.</span>minimize(loss)

<span style=color:#998;font-style:italic># Initializing the variables</span>
init <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>global_variables_initializer()
</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># Merge all summaries into a single operator</span>
merged_summary <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>summary<span style=color:#000;font-weight:700>.</span>merge_all()

</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>batch_size <span style=color:#000;font-weight:700>=</span> <span style=color:#099>100</span>
<span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>batches</span>(data, label,batch_size):
    <span style=color:#000;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(<span style=color:#099>0</span>,data<span style=color:#000;font-weight:700>.</span>shape[<span style=color:#099>0</span>],batch_size):
        <span style=color:#000;font-weight:700>yield</span> [data[i:i<span style=color:#000;font-weight:700>+</span>batch_size], label[i:i<span style=color:#000;font-weight:700>+</span>batch_size]]
</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>summary_writer <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>summary<span style=color:#000;font-weight:700>.</span>FileWriter(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>/content/log</span><span style=color:#d14>&#39;</span>,sess<span style=color:#000;font-weight:700>.</span>graph)
</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># Launch the graph</span>
<span style=color:#000;font-weight:700>with</span> tf<span style=color:#000;font-weight:700>.</span>Session() <span style=color:#000;font-weight:700>as</span> sess:
    sess<span style=color:#000;font-weight:700>.</span>run(init)
    
    total_batches <span style=color:#000;font-weight:700>=</span> <span style=color:#0086b3>len</span>(train)<span style=color:#000;font-weight:700>/</span><span style=color:#000;font-weight:700>/</span>batch_size
    <span style=color:#000;font-weight:700>print</span>(total_batches)
    <span style=color:#000;font-weight:700>for</span> epoch <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(<span style=color:#099>10</span>):
        avg_cost <span style=color:#000;font-weight:700>=</span> <span style=color:#099>0</span>
      
        <span style=color:#998;font-style:italic># Loop over all batches</span>
        batchedData  <span style=color:#000;font-weight:700>=</span> batches(trainSet,trainLabels,batch_size)
        <span style=color:#000;font-weight:700>for</span> batch_data <span style=color:#000;font-weight:700>in</span> batchedData:
            _,c, summary_,acc <span style=color:#000;font-weight:700>=</span> sess<span style=color:#000;font-weight:700>.</span>run([train_op, loss, merged_summary, accuracy], feed_dict<span style=color:#000;font-weight:700>=</span>{X: batch_data[<span style=color:#099>0</span>],Y: batch_data[<span style=color:#099>1</span>]})
            <span style=color:#998;font-style:italic># run summary and accuracy </span>
            <span style=color:#998;font-style:italic>#summary = sess.run([],feed_dict={X: batch_data[0], Y: batch_data[1]})</span>
            
            <span style=color:#998;font-style:italic># Compute average loss</span>
            avg_cost <span style=color:#000;font-weight:700>+</span><span style=color:#000;font-weight:700>=</span> c <span style=color:#000;font-weight:700>/</span> total_batches
            
            <span style=color:#998;font-style:italic># write all the data to the file</span>
            summary_writer<span style=color:#000;font-weight:700>.</span>add_summary(summary_, epoch)
        <span style=color:#998;font-style:italic># Display logs per epoch step</span>
        <span style=color:#000;font-weight:700>if</span> epoch <span style=color:#000;font-weight:700>%</span> <span style=color:#099>1</span> <span style=color:#000;font-weight:700>==</span> <span style=color:#099>0</span>:
            <span style=color:#000;font-weight:700>print</span>(<span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>Epoch:</span><span style=color:#d14>&#34;</span>, <span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>%04d</span><span style=color:#d14>&#39;</span> <span style=color:#000;font-weight:700>%</span> (epoch<span style=color:#000;font-weight:700>+</span><span style=color:#099>1</span>), <span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>cost={:.9f} accuracy={:.2f}</span><span style=color:#d14>%</span><span style=color:#d14>&#34;</span><span style=color:#000;font-weight:700>.</span>format(avg_cost,acc<span style=color:#000;font-weight:700>*</span><span style=color:#099>100</span> ))
            
            
    <span style=color:#000;font-weight:700>print</span>(<span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>Optimization Finished!</span><span style=color:#d14>&#34;</span>)
    <span style=color:#998;font-style:italic># Test model</span>
    pred <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>nn<span style=color:#000;font-weight:700>.</span>softmax(output)  <span style=color:#998;font-style:italic># Apply softmax to logits</span>
    correct_prediction <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>equal(tf<span style=color:#000;font-weight:700>.</span>argmax(pred, <span style=color:#099>1</span>), tf<span style=color:#000;font-weight:700>.</span>argmax(Y, <span style=color:#099>1</span>))
    <span style=color:#998;font-style:italic># Calculate accuracy</span>
    accuracy <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>reduce_mean(tf<span style=color:#000;font-weight:700>.</span>cast(correct_prediction, <span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>float</span><span style=color:#d14>&#34;</span>))
    <span style=color:#000;font-weight:700>print</span>(<span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>Accuracy:</span><span style=color:#d14>&#34;</span>, accuracy<span style=color:#000;font-weight:700>.</span>eval({X: testSet, Y: testLabels}))
</code></pre></div><pre><code>599
Epoch: 0001 cost=343591.474908259 accuracy=84.85%
Epoch: 0002 cost=1363.470883059 accuracy=89.90%
Epoch: 0003 cost=353.140610641 accuracy=93.94%
Epoch: 0004 cost=145.729113662 accuracy=96.97%
Epoch: 0005 cost=71.671148529 accuracy=98.99%
Epoch: 0006 cost=48.250662688 accuracy=96.97%
Epoch: 0007 cost=34.012532867 accuracy=98.99%
Epoch: 0008 cost=32.737998139 accuracy=95.96%
Epoch: 0009 cost=29.617493640 accuracy=98.99%
Epoch: 0010 cost=28.458745954 accuracy=96.97%
Optimization Finished!
Accuracy: 0.93049306
</code></pre><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>tbc<span style=color:#000;font-weight:700>=</span>TensorBoardColab(graph_path<span style=color:#000;font-weight:700>=</span><span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>/content/log</span><span style=color:#d14>&#39;</span>)
</code></pre></div><pre><code>Wait for 8 seconds...
TensorBoard link:
http://4304f716.ngrok.io
</code></pre><p>Now you can copy the above link or click <a href=http://4304f716.ngrok.io>here</a> to visit tensorboard page and explore how the training is done.</p></div></article><button class=floating-button>
<a class=floating-button__link href=https://jdvala.github.io><span>home</span></a></button></div><footer class=post-footer><div class=footer><div>© 2020, Jay Vala. Theme - Origin by Andrey Parfenov</div><div class=footer__socials><a href=www.github.com/jdvala target=_blank class=social-link title="Github link" rel=noopener aria-label="follow on Github——Opens in a new window"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M0 0v24h24V0H0zm14.534 19.59c-.406.078-.534-.171-.534-.384v-2.195c0-.747-.262-1.233-.55-1.481 1.782-.198 3.654-.875 3.654-3.947.0-.874-.311-1.588-.824-2.147.083-.202.357-1.016-.079-2.117.0.0-.671-.215-2.198.82-.639-.18-1.323-.267-2.003-.271-.68.003-1.364.091-2.003.269-1.528-1.035-2.2-.82-2.2-.82-.434 1.102-.16 1.915-.077 2.118-.512.56-.824 1.273-.824 2.147.0 3.064 1.867 3.751 3.645 3.954-.229.2-.436.552-.508 1.07-.457.204-1.614.557-2.328-.666.0.0-.423-.768-1.227-.825.0.0-.78-.01-.055.487.0.0.525.246.889 1.17.0.0.463 1.428 2.688.944v1.489c0 .211-.129.459-.528.385-3.18-1.057-5.472-4.056-5.472-7.59.0-4.419 3.582-8 8-8s8 3.581 8 8c0 3.533-2.289 6.531-5.466 7.59z"/></svg></a></div></div></footer><script src=https://jdvala.github.io/js/script.js></script></body></html>