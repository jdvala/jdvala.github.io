<!doctype html><html lang=en><head><meta charset=utf-8><title>Jay Vala</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Handy dandy notes"><meta property="og:title" content="NLP and ML notes"><meta property="og:description" content="Handy dandy notes"><meta property="og:type" content="website"><meta property="og:url" content="https://jdvala.github.io/posts/2018-11-02-basic-notes/"><meta itemprop=name content="NLP and ML notes"><meta itemprop=description content="Handy dandy notes"><meta name=twitter:card content="summary"><meta name=twitter:title content="NLP and ML notes"><meta name=twitter:description content="Handy dandy notes"><link rel=apple-touch-icon sizes=180x180 href=apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=favicon-32.png><link rel=stylesheet href=https://jdvala.github.io/scss/style.min.d1aa507e320f63a9a89fb4d16c025955cea1564900de1060a4b2d7cabbabcdec.css></head><body><header><div class="header header-frame"><div><h1 class=header__title>NLP and ML notes</h1><div class=header__description>Handy dandy notes</div></div><nav class=header-nav><ul class="header-nav-list header-nav-list--menu"><li class=header-nav-list__item><a class=header-nav-list__link href=/about/><span>About</span></a></li></ul><button class=header-nav-list__nav-btn>navigation</button></nav><button class=mb-header__menu-btn>
<span class=mb-header__menu-btn-line></span>
<span class=mb-header__menu-btn-line></span>
<span class=mb-header__menu-btn-line></span></button></div><nav id=mobile-header-nav class=mb-header-nav><button class="mb-header-nav__close-btn flex-center"><svg class="mb-header-nav__svg-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="32" height="32"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/><path d="M0 0h24v24H0z" fill="none"/></svg></button><div class=mb-header-nav__wrapper><div class=mb-header-nav__container><svg width="240" height="72" viewBox="0 0 240 72" class="mb-header-nav__title"><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle">Tags</text></svg><ul class=mb-header-nav-list><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/python/>python</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/text/>text</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/nlp/>nlp</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/analysis/>analysis</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/word2vec/>word2vec</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/pos/>POS</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/tf-idf/>TF-IDF</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/probability/>probability</a></li></ul></div><div class=mb-header-nav__container><svg width="240" height="72" viewBox="0 0 240 72" class="mb-header-nav__title"><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle">Menu</text></svg><ul class=mb-header-nav-list><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=/about/>About</a></li></ul></div></div></nav></header><div id=content><article class=post><div class=post-content><h1 id=notes-natural-language-processing-and-machine-learning>Notes: Natural Language Processing and Machine Learning</h1><p><strong>Question:</strong> What are <em>word embedding</em> or what are <em>word vectors</em>?</p><p><strong>Answer:</strong> Word embedding are learned representation for text where words that have the same meaning have a similar representation. Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.</p><p><strong>Question</strong> What is POS tagging?</p><p><strong>Answer:</strong> Parts of Speech(POS) tagging is a method of assigning Part of Speech such as <em>Noun</em>, <em>Verb</em>, <em>adjectivte</em> etc to each word of the text.</p><p><strong>Question:</strong> What is TF-IDF matrix?</p><p><strong>Answer:</strong> <em>Term Frequency Inverse Document Frequency(TF-IDF)</em> is a numerical statistical method that shows how important a word is to a document in a corpus. It contains two terms,</p><ul><li><p><strong>Term Frequecy</strong>: It is the measure of how frequently a term appears in a document. Every document is of different length and hence it may happen that a term might appear in several times in a document, so as a measure of normalization we divide the term appearence in document by the length of document.</p></li><li><p><strong>Inverse Document Frequency</strong>: It is the meausre of how important a term is. While calculating Term Frequency every term is given same importance, but it is evedent that terms like &ldquo;is&rdquo;, &ldquo;the&rdquo; don&rsquo;t contribute and hence they need to be scaled down and the rare once have to scaledup</p><ul><li>TF = (Number of times term appear in document/Number of document with term in it)</li><li>IDF = log_e(Total Number of documents/ Number of documents with the term in it)</li></ul></li></ul><p><strong>Question:</strong> What is Zip&rsquo;s Law?</p><p><strong>Answer:</strong> Zip&rsquo;s Law states that <em>&ldquo;If we count the frequency of words and rank them according to the frequency of occurance we can explore relationship between the frequency and its frequency in the list known as rank &ldquo;r&rdquo;, then Zip&rsquo;s law says that&rdquo;</em>
$$ f \propto \frac{1}{r}$$</p><p>or in other words, there is a constant $$ k $$ such that
$$ f . r= k$$</p><p><strong>Question:</strong> What is a probability distribution function?</p><p><strong>Answer:</strong> A probability function or probability distribution function distributes a probability mass of 1 throughout given sample space.</p><p><strong>Question:</strong> What is Uniform Distribution?</p><p><strong>Answer:</strong> A situation where each outcome is equaly likely is called a <em>Uniform Distribution</em>.</p><h2 id=mathematical-definations>Mathematical Definations:</h2><ul><li><strong>Conditional Probability:</strong> It is updated probability of an event given some prior knowledge.
The conditinal probability of an event A given that event B has occured is:</li></ul><p>$$ P(A|B) = \frac{P(A\bigcap B)}{P(B)}$$</p><p>If $P(B) = 0$ we have</p><p>$$P(A \bigcap B) = P(B)P(A|B) = P(A)P(B|A)$$</p><ul><li><p><strong>Prior Probability:</strong> The probability of an event before we consider our additional knowledge.</p></li><li><p><strong>Posterior Probability:</strong> The new probability that results from using of additional knowledge.</p></li><li><p><strong>Chain Rule:</strong></p></li></ul><p>$$P(A_4,A_3,A_2,A_1) = P(A_4|A_3,A_2,A_1).P(A_3|A_2,A_1).P(A_2|A_1).P(A_1)$$</p><blockquote><p><em>Example: Urn 1 has 1 black ball and 2 white balls and Urn 2 has 1 black ball and 3 white balls. Suppose we pick an urn at random and then select a ball from that urnUrn 1 has 1 black ball and 2 white balls and Urn 2 has 1 black ball and 3 white balls. Suppose we pick an urn at random, then what is the probability that we choose a black ball from urn 2</em></p></blockquote><p>To answer the above question we can use the chain rule.</p><p>Let $P(A)$ be the probabilty of picking urn 2, Hence</p><p>$$P(A) = \frac{1}{2}$$</p><p>Then Let $P(B|A)$ be the probability of picking black ball from urn 2.</p><p>$$P(B|A) =\frac{1}{4}$$</p><p>Hence, using chain rule we can calculate probaility $P(A,B)$</p><p>$$P(A,B) = P(B|A).P(A)$$</p><p>$$P(A,B) = \frac{1}{4} * \frac{1}{2}$$</p><p>$$P(A,B) = \frac{1}{8}$$</p></div></article><button class=floating-button>
<a class=floating-button__link href=https://jdvala.github.io><span>home</span></a></button></div><footer class=post-footer><div class=footer><div>© 2020, Jay Vala. Theme - Origin by Andrey Parfenov</div><div class=footer__socials><a href=www.github.com/jdvala target=_blank class=social-link title="Github link" rel=noopener aria-label="follow on Github——Opens in a new window"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M0 0v24h24V0H0zm14.534 19.59c-.406.078-.534-.171-.534-.384v-2.195c0-.747-.262-1.233-.55-1.481 1.782-.198 3.654-.875 3.654-3.947.0-.874-.311-1.588-.824-2.147.083-.202.357-1.016-.079-2.117.0.0-.671-.215-2.198.82-.639-.18-1.323-.267-2.003-.271-.68.003-1.364.091-2.003.269-1.528-1.035-2.2-.82-2.2-.82-.434 1.102-.16 1.915-.077 2.118-.512.56-.824 1.273-.824 2.147.0 3.064 1.867 3.751 3.645 3.954-.229.2-.436.552-.508 1.07-.457.204-1.614.557-2.328-.666.0.0-.423-.768-1.227-.825.0.0-.78-.01-.055.487.0.0.525.246.889 1.17.0.0.463 1.428 2.688.944v1.489c0 .211-.129.459-.528.385C6.292 18.533 4 15.534 4 12c0-4.419 3.582-8 8-8s8 3.581 8 8c0 3.533-2.289 6.531-5.466 7.59z"/></svg></a></div></div></footer><script src=https://jdvala.github.io/js/script.js></script></body></html>