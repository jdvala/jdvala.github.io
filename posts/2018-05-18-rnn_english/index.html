<!doctype html><html lang=en><head><meta charset=utf-8><title>Jay Vala</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Training Recurrent Neural Networks on the english EU summaries data"><meta property="og:title" content="(UPDATED)Sequence Classification using Recurrent Neural Networks"><meta property="og:description" content="Training Recurrent Neural Networks on the english EU summaries data"><meta property="og:type" content="website"><meta property="og:url" content="https://jdvala.github.io/posts/2018-05-18-rnn_english/"><meta itemprop=name content="(UPDATED)Sequence Classification using Recurrent Neural Networks"><meta itemprop=description content="Training Recurrent Neural Networks on the english EU summaries data"><meta name=twitter:card content="summary"><meta name=twitter:title content="(UPDATED)Sequence Classification using Recurrent Neural Networks"><meta name=twitter:description content="Training Recurrent Neural Networks on the english EU summaries data"><link rel=apple-touch-icon sizes=180x180 href=apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=favicon-32.png><link rel=stylesheet href=https://jdvala.github.io/scss/style.min.d1aa507e320f63a9a89fb4d16c025955cea1564900de1060a4b2d7cabbabcdec.css></head><body><header><div class="header header-frame"><div><h1 class=header__title>(UPDATED)Sequence Classification using Recurrent Neural Networks</h1><div class=header__description>Training Recurrent Neural Networks on the english EU summaries data</div></div><nav class=header-nav><ul class="header-nav-list header-nav-list--menu"><li class=header-nav-list__item><a class=header-nav-list__link href=/about/><span>About</span></a></li></ul><button class=header-nav-list__nav-btn>navigation</button></nav><button class=mb-header__menu-btn>
<span class=mb-header__menu-btn-line></span>
<span class=mb-header__menu-btn-line></span>
<span class=mb-header__menu-btn-line></span></button></div><nav id=mobile-header-nav class=mb-header-nav><button class="mb-header-nav__close-btn flex-center"><svg class="mb-header-nav__svg-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="32" height="32"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/><path d="M0 0h24v24H0z" fill="none"/></svg></button><div class=mb-header-nav__wrapper><div class=mb-header-nav__container><svg width="240" height="72" viewBox="0 0 240 72" class="mb-header-nav__title"><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle">Tags</text></svg><ul class=mb-header-nav-list><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/python/>python</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/text/>text</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/dataset/>dataset</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/nlp/>nlp</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/ai/>AI</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/rnn/>RNN</a></li></ul></div><div class=mb-header-nav__container><svg width="240" height="72" viewBox="0 0 240 72" class="mb-header-nav__title"><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle">Menu</text></svg><ul class=mb-header-nav-list><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=/about/>About</a></li></ul></div></div></nav></header><div id=content><article class=post><div class=post-content><h3 id=update>Update:</h3><p>Please go to the end of the post to see the updated results.</p><h3 id=aim>Aim:</h3><p>The aim of this model is to classify sentences. I am going to use Recurrent Neural Network for this purpose, specifically LSTMs. LSTMs have an advantage compared to simple RNN cell or GRU cell. You can learn about <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>LSTMs</a> and <a href=http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/>RNNs</a>, There is no point on going in detail here. May be I will write another post wherein I will describe RNNs and LSTMs more intutively, but for today these two links will suffice.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Import dependencies</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>gensim.models</span> <span style=color:#000;font-weight:700>import</span> Word2Vec
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>keras</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>numpy</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>np</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>pandas</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>pd</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>keras.datasets</span> <span style=color:#000;font-weight:700>import</span> imdb
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>keras.models</span> <span style=color:#000;font-weight:700>import</span> Sequential
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>keras.layers</span> <span style=color:#000;font-weight:700>import</span> Dense
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>keras.layers</span> <span style=color:#000;font-weight:700>import</span> LSTM
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># Ignoring Gensim Warinings</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>warnings</span>
</span></span><span style=display:flex><span>warnings<span style=color:#000;font-weight:700>.</span>filterwarnings(<span style=color:#d14>&#34;ignore&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Let&#39;s load our model</span>
</span></span><span style=display:flex><span>model <span style=color:#000;font-weight:700>=</span> Word2Vec<span style=color:#000;font-weight:700>.</span>load(<span style=color:#d14>&#39;/home/jay/Saved_Models/english/english&#39;</span>)
</span></span></code></pre></div><p>Firstly, we can not use this model directly into tensorflow. So what do we do now? Its simple we have to convert this model into numpy matrix that we can use to train our model.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Create a matrix with the shape of Length of Vocab * total Dimensions </span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># (for this model it will be 6442*4000, we have 6442 words in 4000 dimension)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>embedding_matrix <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((<span style=color:#0086b3>len</span>(model<span style=color:#000;font-weight:700>.</span>wv<span style=color:#000;font-weight:700>.</span>vocab), <span style=color:#099>4000</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(<span style=color:#0086b3>len</span>(model<span style=color:#000;font-weight:700>.</span>wv<span style=color:#000;font-weight:700>.</span>vocab)):
</span></span><span style=display:flex><span>    embedding_vector <span style=color:#000;font-weight:700>=</span> model<span style=color:#000;font-weight:700>.</span>wv[model<span style=color:#000;font-weight:700>.</span>wv<span style=color:#000;font-weight:700>.</span>index2word[i]]
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>if</span> embedding_vector <span style=color:#000;font-weight:700>is</span> <span style=color:#000;font-weight:700>not</span> <span style=color:#000;font-weight:700>None</span>:
</span></span><span style=display:flex><span>        embedding_matrix[i] <span style=color:#000;font-weight:700>=</span> embedding_vector
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>embedding_matrix<span style=color:#000;font-weight:700>.</span>shape
</span></span></code></pre></div><pre><code>(6442, 4000)
</code></pre><p>Thats about right, So we have loaded the model and created the embedding matrix to be used insted of embedding layer in Keras.</p><p>The next step would be to prepare the data to be feed into the model(LSTM), but we have prepared dataset for feeding to a neural network, so what is this now?? Well the dataset we created is not suitable for feeding it to neural network, there are a few steps that needs to be done before running the all fancy LSTM.</p><p>LSTMs take a fixed lenght sentences and also it will not take raw alpha numeric values, the words are to be converted into numbers, also all the sentences are to made with fixed lenght.</p><p>In the next step we are going to do that.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Lets load the dataset using pandas</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dataset <span style=color:#000;font-weight:700>=</span> pd<span style=color:#000;font-weight:700>.</span>read_csv(<span style=color:#d14>&#39;/home/jay/Data_Set_Creation/Data_to_Use.csv&#39;</span>) 
</span></span><span style=display:flex><span>dataset<span style=color:#000;font-weight:700>.</span>head()
</span></span></code></pre></div><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>Sentence</th><th>Label</th><th>Numerical_Label</th></tr></thead><tbody><tr><th>0</th><td>summary the directive seek ensure eu country f...</td><td>transport</td><td>0</td></tr><tr><th>1</th><td>mlc set minimum global standard ensure right s...</td><td>transport</td><td>0</td></tr><tr><th>2</th><td>pron also seek limit social dump secure fair c...</td><td>transport</td><td>0</td></tr><tr><th>3</th><td>the main point new directive follow</td><td>transport</td><td>0</td></tr><tr><th>4</th><td>monitor compliance eu country introduce effect...</td><td>transport</td><td>0</td></tr></tbody></table></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># We can not use this dataframe so lets transform them to list</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sent <span style=color:#000;font-weight:700>=</span> dataset<span style=color:#000;font-weight:700>.</span>Sentence<span style=color:#000;font-weight:700>.</span>tolist()
</span></span><span style=display:flex><span>label <span style=color:#000;font-weight:700>=</span> dataset<span style=color:#000;font-weight:700>.</span>Numerical_Label<span style=color:#000;font-weight:700>.</span>tolist()
</span></span></code></pre></div><p>For converting the words to numerical representations we have to tokenize the words</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Using Keras tokenizer</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#000;font-weight:700>=</span> keras<span style=color:#000;font-weight:700>.</span>preprocessing<span style=color:#000;font-weight:700>.</span>text<span style=color:#000;font-weight:700>.</span>Tokenizer()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># Tokenizing the sentences (This process may take some time depending on your corpus size)</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#000;font-weight:700>.</span>fit_on_texts(sent)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Lets see what our vocabulary size is</span>
</span></span><span style=display:flex><span>vocab_size <span style=color:#000;font-weight:700>=</span> <span style=color:#0086b3>len</span>(tokenizer<span style=color:#000;font-weight:700>.</span>word_index) <span style=color:#000;font-weight:700>+</span> <span style=color:#099>1</span>   <span style=color:#998;font-style:italic># We are adding 1 here because it takes indexing from zero</span>
</span></span><span style=display:flex><span>vocab_size
</span></span></code></pre></div><pre><code>16024
</code></pre><p>We have 16,387 words in our vocabulary, Now let&rsquo;s encode the sentences to its repecitve numbers, if you don&rsquo;t get what I say right now be patitent, I will show an example which will clear it out.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Sentence encoding</span>
</span></span><span style=display:flex><span>sent_encoded <span style=color:#000;font-weight:700>=</span> tokenizer<span style=color:#000;font-weight:700>.</span>texts_to_sequences(sent)
</span></span></code></pre></div><p>So, back to what I was expalining how encoding works, Let&rsquo;s say you have a document with only one sentence <strong>The quick brown fox jumps over the lazy dog</strong> and you want to use this sentence for training a RNN(LSTM), but the LSTM doesn&rsquo;t understand this, it only understands numbers, so how to go on with this? Well its fairly simple, you tokenize the sentence and give each word a unique number.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Lets visualize whatever is written above</span>
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>&#34;</span><span style=color:#d14>{}</span><span style=color:#d14> </span><span style=color:#d14>\n</span><span style=color:#d14> </span><span style=color:#d14>{}</span><span style=color:#d14>&#34;</span><span style=color:#000;font-weight:700>.</span>format(sent[<span style=color:#099>1</span>], sent_encoded[<span style=color:#099>1</span>]))
</span></span></code></pre></div><pre><code>mlc set minimum global standard ensure right seafarer decent live working condition irrespective nationality irrespective flag ship serve : [7243, 42, 372, 364, 141, 31, 46, 2048, 2921, 870, 696, 99, 1674, 1577, 1674, 1218, 374, 971]
</code></pre><p>What the hell is that? I don&rsquo;t understand. Well it means the word <em>mlc</em> is at index <em>7243</em> in the dictonary of our model. Don&rsquo;t believe me? Let&rsquo;s verify that.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>word_dict <span style=color:#000;font-weight:700>=</span> tokenizer<span style=color:#000;font-weight:700>.</span>word_index
</span></span><span style=display:flex><span>word_dict[<span style=color:#d14>&#39;mlc&#39;</span>]
</span></span></code></pre></div><pre><code>7243
</code></pre><p>Well well well, I should say I told you so. Now remember as we said that we need to make all the sentences in our dataset of a fixed length. This raises more questions, what is padding? how to do it?. Keras provides with functions to do it. So what is padding?</p><p>Padding is used to ensure that all sequences in a list have the same length, So by default <code>0</code> is added at the end.
Example: We have a squence <code>[1, 2, 3], [3, 4, 5, 6], [7, 8]</code> which are all of different length. So if we pad it it will be <code>[1, 2, 3, 0], [3, 4, 5, 6], [7, 8, 0, 0]</code>, Note that we have added zeros at the end of the sentences, its called <em>post padding</em>, there is also <em>pre padding</em> where the zeros are added at the start of the squence.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># padding all the encoded sequences</span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># Before padding we need to define what should be the maximum padding length, for that we need to check what</span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># is the maximum length of sentence in out dataset, we can simply do it by</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>heapq</span>
</span></span><span style=display:flex><span>heapq<span style=color:#000;font-weight:700>.</span>nlargest(<span style=color:#099>5</span>, sent, key<span style=color:#000;font-weight:700>=</span><span style=color:#0086b3>len</span>)
</span></span></code></pre></div><pre><code>['lithuania adoption community acquisi lithuania adoption community acquisi lithuaniaarchif lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives lithuaniaarchives',
 'slovakia adoption community acquisi slovakia adoption community acquisi slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives slovakiaarchives',
 'bulgaria adoption community acquisi bulgaria adoption community acquisi bulgariaarchives bulgariaarchives bulgariaarchives bulgariaarchives bulgariarchives bulgariaarchives bulgariaarchives bulgariaarchives bulgariaarchives bulgariaarchives bulgariaarchives bulgariaarchives',
 'hungary adoption community acquisi hungary adoption community acquisi hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives hungaryarchives',
 'estonia adoption community acquis estonia adoption community acquis estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives estoniaarchives']
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#0086b3>sorted</span>(sent,key<span style=color:#000;font-weight:700>=</span><span style=color:#0086b3>len</span>)
</span></span></code></pre></div><pre><code>['aid amount set euro per box',
 'pron aim ensure pron safe use',
 'pron valid ten year may renew',
 'pron aim ensure pron safe use',
 'eu large aid donor global oda',
 'article set value upon eu base',
 'less red tape less risky trial',
 'datum may keep file three year',
 'one year term office may renew',
 'datum may keep file three year',
 'datum may keep file three year',
 'time sector may add pron scope',
 'datum may keep file three year',
 'article set value upon eu base',
 'date bank issue euro note coin',
 'community aid eur kg type milk',
 'diet calf feed least twice day',
 'word levy use cover tax charge',
  ...]
</code></pre><p>This looks promising, so I would like to go ahead with padding I described earlier. Keras provides simple padding function for us to use. Before we use that we have to define <code>max_lenght</code>, for padding we need to define the maximum size to which we pad the sequence. This is our <code>max_length</code>. And it is highly advisable that we use the maximum length of sentence from our corpus.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Defining max_length</span>
</span></span><span style=display:flex><span>max_sent <span style=color:#000;font-weight:700>=</span> <span style=color:#0086b3>max</span>(sent, key<span style=color:#000;font-weight:700>=</span><span style=color:#0086b3>len</span>)    <span style=color:#998;font-style:italic># Get the longest sentence in the list</span>
</span></span><span style=display:flex><span>max_length <span style=color:#000;font-weight:700>=</span> <span style=color:#0086b3>len</span>(max_sent<span style=color:#000;font-weight:700>.</span>split())  <span style=color:#998;font-style:italic># split it and set the max_length</span>
</span></span><span style=display:flex><span>max_length
</span></span></code></pre></div><pre><code>20
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Start padding with the max_length</span>
</span></span><span style=display:flex><span>padded_sents <span style=color:#000;font-weight:700>=</span> keras<span style=color:#000;font-weight:700>.</span>preprocessing<span style=color:#000;font-weight:700>.</span>sequence<span style=color:#000;font-weight:700>.</span>pad_sequences(sent_encoded, maxlen<span style=color:#000;font-weight:700>=</span>max_length, padding<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;post&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># Note: We are using post padding</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>padded_sents<span style=color:#000;font-weight:700>.</span>shape
</span></span></code></pre></div><pre><code>(128075, 20)
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Initializing weights matrix of the embedding layer</span>
</span></span><span style=display:flex><span>embedding_layer <span style=color:#000;font-weight:700>=</span> keras<span style=color:#000;font-weight:700>.</span>layers<span style=color:#000;font-weight:700>.</span>Embedding(vocab_size, <span style=color:#099>4000</span>, weights<span style=color:#000;font-weight:700>=</span>[embedding_matrix], input_length<span style=color:#000;font-weight:700>=</span> max_length, trainable<span style=color:#000;font-weight:700>=</span><span style=color:#000;font-weight:700>False</span>)
</span></span></code></pre></div><p><strong>NOTE</strong> that I have delibrately kept the <em>flag trainable=False</em>, because I have already trained the model once and I don&rsquo;t want to train or change the weight matrix any further.</p><h3 id=spliting-the-dataset-into-test-and-train-sets>Spliting the dataset into test and train sets</h3><p>We have the whole dataset but for supervised training of a deep neural network we need training and testing set.
Also in our dataset we have all the data stored squentially, meaning that all the classes are clustered which is not ideal, so firstly we will suffle the dataset then divide it into training and testing sets.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Split the data into test and train</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>sklearn.model_selection</span> <span style=color:#000;font-weight:700>import</span> train_test_split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_set, test_set, train_label, test_label <span style=color:#000;font-weight:700>=</span> train_test_split(padded_sents, label,test_size<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.33</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_set<span style=color:#000;font-weight:700>.</span>shape
</span></span></code></pre></div><pre><code>(85810, 20)
</code></pre><p><strong>NOTE:</strong> Keras expects numpy arrays, if you just provide it the training data in form of list, it will throw an error, so covert the list to arrays</p><p>Prepraing Labels is another task at hand, which needs to be done before feeding data into the neural network, agian using sklearn one hot encoding will do the trick</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>sklearn.preprocessing</span> <span style=color:#000;font-weight:700>import</span> OneHotEncoder
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># Define one_hot_encoder object</span>
</span></span><span style=display:flex><span>onehot_encoder <span style=color:#000;font-weight:700>=</span> OneHotEncoder(sparse<span style=color:#000;font-weight:700>=</span><span style=color:#000;font-weight:700>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_labels <span style=color:#000;font-weight:700>=</span> onehot_encoder<span style=color:#000;font-weight:700>.</span>fit_transform(np<span style=color:#000;font-weight:700>.</span>reshape(train_label,(<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>,<span style=color:#099>1</span>)))
</span></span><span style=display:flex><span>test_labels <span style=color:#000;font-weight:700>=</span> onehot_encoder<span style=color:#000;font-weight:700>.</span>fit_transform(np<span style=color:#000;font-weight:700>.</span>reshape(test_label,(<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>,<span style=color:#099>1</span>)))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_labels<span style=color:#000;font-weight:700>.</span>shape
</span></span></code></pre></div><pre><code>(85810, 32)
</code></pre><h3 id=details-about-the-model>Details about the model</h3><p>Now that we have every thing ready we can start building the model, but there are a few questions that are needed to be answered before making the model, How many layers we are gonna use, what would be the activation function, how many neurons in each layer, what kind of regularization and what not. So I am going to build a very basic model and see how it does on the data. After the first results, I will be in a state to make changes.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Create sequential model</span>
</span></span><span style=display:flex><span>model <span style=color:#000;font-weight:700>=</span> Sequential()
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>add(embedding_layer)   <span style=color:#998;font-style:italic># adding embedding layer, which we have defined earlier</span>
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>add(LSTM(<span style=color:#099>50</span>, return_sequences<span style=color:#000;font-weight:700>=</span><span style=color:#000;font-weight:700>True</span>)) <span style=color:#998;font-style:italic># LSTM layer with 50 units</span>
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>add(LSTM(<span style=color:#099>50</span>, return_sequences<span style=color:#000;font-weight:700>=</span><span style=color:#000;font-weight:700>True</span>)) <span style=color:#998;font-style:italic># LSTM layer with 50 units</span>
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>add(LSTM(<span style=color:#099>50</span>)) <span style=color:#998;font-style:italic># LSTM layer with 50 units</span>
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>add(Dense(<span style=color:#099>32</span>, activation<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;softmax&#39;</span>))
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>compile(loss<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;categorical_crossentropy&#39;</span>, optimizer<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;adam&#39;</span>, metrics<span style=color:#000;font-weight:700>=</span>[<span style=color:#d14>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(model<span style=color:#000;font-weight:700>.</span>summary())
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>fit(train_set, train_labels, validation_split<span style=color:#000;font-weight:700>=</span><span style=color:#099>.1</span>, epochs<span style=color:#000;font-weight:700>=</span><span style=color:#099>3</span>, batch_size<span style=color:#000;font-weight:700>=</span><span style=color:#099>64</span>, verbose<span style=color:#000;font-weight:700>=</span><span style=color:#099>1</span>)
</span></span></code></pre></div><pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 20, 4000)          64096000  
_________________________________________________________________
lstm_16 (LSTM)               (None, 20, 50)            810200    
_________________________________________________________________
lstm_17 (LSTM)               (None, 20, 50)            20200     
_________________________________________________________________
lstm_18 (LSTM)               (None, 50)                20200     
_________________________________________________________________
dense_6 (Dense)              (None, 32)                1632      
=================================================================
Total params: 64,948,232
Trainable params: 852,232
Non-trainable params: 64,096,000
_________________________________________________________________
None
Train on 77229 samples, validate on 8581 samples
Epoch 1/3
77229/77229 [==============================] - 309s 4ms/step - loss: 2.6437 - acc: 0.2746 - val_loss: 2.2583 - val_acc: 0.3832
Epoch 2/3
77229/77229 [==============================] - 344s 4ms/step - loss: 2.0753 - acc: 0.4300 - val_loss: 2.0260 - val_acc: 0.4301
Epoch 3/3
77229/77229 [==============================] - 392s 5ms/step - loss: 1.8524 - acc: 0.4794 - val_loss: 1.9480 - val_acc: 0.4477





&lt;keras.callbacks.History at 0x7ff3d8408128&gt;
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>scores <span style=color:#000;font-weight:700>=</span> model<span style=color:#000;font-weight:700>.</span>evaluate(test_set, test_labels, verbose<span style=color:#000;font-weight:700>=</span><span style=color:#099>1</span>)
</span></span></code></pre></div><pre><code>42265/42265 [==============================] - 107s 3ms/step
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>&#34;Accuracy: </span><span style=color:#d14>%.2f%%</span><span style=color:#d14>&#34;</span> <span style=color:#000;font-weight:700>%</span> (scores[<span style=color:#099>1</span>]<span style=color:#000;font-weight:700>*</span><span style=color:#099>100</span>))
</span></span></code></pre></div><pre><code>Accuracy: 45.04%
</code></pre><p>The process was painstakingly slow, hence only performed 3 epochs, but we can conclued that there is still a chance of improvement if I have enough resources or a lot of patience(which I don&rsquo;t have). So I will update this post with the updated results as soon as I have them. Till then, Have a great day.</p><h2 id=update-1>Update</h2><p>I managed to get a gpu enabled instance on google cloud. So I decided to run the algorithm there. The result is not that good but afterall its a trial which shed light on my dataset so I decided to update this post and pin in the results.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Train on <span style=color:#099>77229</span> samples, validate on <span style=color:#099>8581</span> samples
</span></span><span style=display:flex><span>2018-05-22 17:35:16.036372: I tensorflow/core/platform/cpu_feature_guard.cc:140<span style=color:#000;font-weight:700>]</span> Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
</span></span><span style=display:flex><span>2018-05-22 17:35:16.513689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898<span style=color:#000;font-weight:700>]</span> successful NUMA node <span style=color:#0086b3>read</span> from SysFS had negative value <span style=color:#000;font-weight:700>(</span>-1<span style=color:#000;font-weight:700>)</span>, but there must be at least one NUMA node, so returning NUMA node zero
</span></span><span style=display:flex><span>2018-05-22 17:35:16.514222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356<span style=color:#000;font-weight:700>]</span> Found device <span style=color:#099>0</span> with properties: 
</span></span><span style=display:flex><span>name: Tesla K80 major: <span style=color:#099>3</span> minor: <span style=color:#099>7</span> memoryClockRate<span style=color:#000;font-weight:700>(</span>GHz<span style=color:#000;font-weight:700>)</span>: 0.8235
</span></span><span style=display:flex><span>pciBusID: 0000:00:04.0
</span></span><span style=display:flex><span>totalMemory: 11.17GiB freeMemory: 11.09GiB
</span></span><span style=display:flex><span>2018-05-22 17:35:16.514250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435<span style=color:#000;font-weight:700>]</span> Adding visible gpu devices: <span style=color:#099>0</span>
</span></span><span style=display:flex><span>2018-05-22 17:35:29.328739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923<span style=color:#000;font-weight:700>]</span> Device interconnect StreamExecutor with strength <span style=color:#099>1</span> edge matrix:
</span></span><span style=display:flex><span>2018-05-22 17:35:29.328808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929<span style=color:#000;font-weight:700>]</span>      <span style=color:#099>0</span> 
</span></span><span style=display:flex><span>2018-05-22 17:35:29.328818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942<span style=color:#000;font-weight:700>]</span> 0:   N 
</span></span><span style=display:flex><span>2018-05-22 17:35:29.379302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053<span style=color:#000;font-weight:700>]</span> Created TensorFlow device <span style=color:#000;font-weight:700>(</span>/job:localhost/replica:0/task:0/device:GPU:0 with <span style=color:#099>10747</span> MB memory<span style=color:#000;font-weight:700>)</span> -&gt; physical GPU <span style=color:#000;font-weight:700>(</span>device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7<span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>Epoch 1/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 148s 2ms/step - loss: 2.4540 - acc: 0.3338 - val_loss: 2.0469 - val_acc: 0.4494
</span></span><span style=display:flex><span>Epoch 2/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 126s 2ms/step - loss: 1.8124 - acc: 0.5052 - val_loss: 1.8633 - val_acc: 0.4860
</span></span><span style=display:flex><span>Epoch 3/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 126s 2ms/step - loss: 1.5552 - acc: 0.5599 - val_loss: 1.8173 - val_acc: 0.4929
</span></span><span style=display:flex><span>Epoch 4/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 127s 2ms/step - loss: 1.3846 - acc: 0.5952 - val_loss: 1.8096 - val_acc: 0.4868
</span></span><span style=display:flex><span>Epoch 5/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 124s 2ms/step - loss: 1.2465 - acc: 0.6237 - val_loss: 1.8556 - val_acc: 0.4905
</span></span><span style=display:flex><span>Epoch 6/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 126s 2ms/step - loss: 1.1322 - acc: 0.6489 - val_loss: 1.9120 - val_acc: 0.4833
</span></span><span style=display:flex><span>Epoch 7/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 127s 2ms/step - loss: 1.0355 - acc: 0.6686 - val_loss: 1.9496 - val_acc: 0.4738
</span></span><span style=display:flex><span>Epoch 8/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.9489 - acc: 0.6863 - val_loss: 2.0237 - val_acc: 0.4705
</span></span><span style=display:flex><span>Epoch 9/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 127s 2ms/step - loss: 0.8778 - acc: 0.7015 - val_loss: 2.1231 - val_acc: 0.4698
</span></span><span style=display:flex><span>Epoch 10/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.8160 - acc: 0.7145 - val_loss: 2.1606 - val_acc: 0.4650
</span></span><span style=display:flex><span>Epoch 11/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 129s 2ms/step - loss: 0.7668 - acc: 0.7255 - val_loss: 2.2607 - val_acc: 0.4594
</span></span><span style=display:flex><span>Epoch 12/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 127s 2ms/step - loss: 0.7277 - acc: 0.7324 - val_loss: 2.3309 - val_acc: 0.4572
</span></span><span style=display:flex><span>Epoch 13/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 126s 2ms/step - loss: 0.6896 - acc: 0.7437 - val_loss: 2.4108 - val_acc: 0.4551
</span></span><span style=display:flex><span>Epoch 14/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 125s 2ms/step - loss: 0.6581 - acc: 0.7490 - val_loss: 2.4212 - val_acc: 0.4541
</span></span><span style=display:flex><span>Epoch 15/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 127s 2ms/step - loss: 0.6302 - acc: 0.7557 - val_loss: 2.4824 - val_acc: 0.4454
</span></span><span style=display:flex><span>Epoch 16/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.6087 - acc: 0.7588 - val_loss: 2.5772 - val_acc: 0.4463
</span></span><span style=display:flex><span>Epoch 17/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.5889 - acc: 0.7633 - val_loss: 2.6004 - val_acc: 0.4496
</span></span><span style=display:flex><span>Epoch 18/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.5739 - acc: 0.7672 - val_loss: 2.6675 - val_acc: 0.4369
</span></span><span style=display:flex><span>Epoch 19/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 129s 2ms/step - loss: 0.5528 - acc: 0.7719 - val_loss: 2.7079 - val_acc: 0.4402
</span></span><span style=display:flex><span>Epoch 20/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 127s 2ms/step - loss: 0.5413 - acc: 0.7748 - val_loss: 2.7494 - val_acc: 0.4378
</span></span><span style=display:flex><span>Epoch 21/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 126s 2ms/step - loss: 0.5309 - acc: 0.7774 - val_loss: 2.8162 - val_acc: 0.4357
</span></span><span style=display:flex><span>Epoch 22/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 125s 2ms/step - loss: 0.5209 - acc: 0.7780 - val_loss: 2.8276 - val_acc: 0.4360
</span></span><span style=display:flex><span>Epoch 23/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.5125 - acc: 0.7803 - val_loss: 2.8808 - val_acc: 0.4301
</span></span><span style=display:flex><span>Epoch 24/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.4969 - acc: 0.7844 - val_loss: 2.8847 - val_acc: 0.4360
</span></span><span style=display:flex><span>Epoch 25/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 127s 2ms/step - loss: 0.4885 - acc: 0.7848 - val_loss: 2.9314 - val_acc: 0.4304
</span></span><span style=display:flex><span>Epoch 26/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.4869 - acc: 0.7844 - val_loss: 2.9672 - val_acc: 0.4328
</span></span><span style=display:flex><span>Epoch 27/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.4778 - acc: 0.7890 - val_loss: 3.0229 - val_acc: 0.4336
</span></span><span style=display:flex><span>Epoch 28/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.4750 - acc: 0.7870 - val_loss: 3.0632 - val_acc: 0.4298
</span></span><span style=display:flex><span>Epoch 29/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.4697 - acc: 0.7894 - val_loss: 3.0448 - val_acc: 0.4314
</span></span><span style=display:flex><span>Epoch 30/30
</span></span><span style=display:flex><span>77229/77229 <span style=color:#000;font-weight:700>[==============================]</span> - 128s 2ms/step - loss: 0.4620 - acc: 0.7892 - val_loss: 3.0588 - val_acc: 0.4236
</span></span><span style=display:flex><span>42265/42265 <span style=color:#000;font-weight:700>[==============================]</span> - 37s 869us/step
</span></span><span style=display:flex><span>Accuracy: 42.71%
</span></span><span style=display:flex><span>             precision    recall  f1-score   support
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>          <span style=color:#099>0</span>       0.58      0.57      0.58      <span style=color:#099>2703</span>
</span></span><span style=display:flex><span>          <span style=color:#099>1</span>       0.26      0.31      0.28       <span style=color:#099>438</span>
</span></span><span style=display:flex><span>          <span style=color:#099>2</span>       0.36      0.41      0.38      <span style=color:#099>1530</span>
</span></span><span style=display:flex><span>          <span style=color:#099>3</span>       0.36      0.38      0.37      <span style=color:#099>3955</span>
</span></span><span style=display:flex><span>          <span style=color:#099>4</span>       0.62      0.60      0.61      <span style=color:#099>4794</span>
</span></span><span style=display:flex><span>          <span style=color:#099>5</span>       0.36      0.34      0.35      <span style=color:#099>1536</span>
</span></span><span style=display:flex><span>          <span style=color:#099>6</span>       0.69      0.58      0.63      <span style=color:#099>1712</span>
</span></span><span style=display:flex><span>          <span style=color:#099>7</span>       0.34      0.22      0.27       <span style=color:#099>498</span>
</span></span><span style=display:flex><span>          <span style=color:#099>8</span>       0.29      0.33      0.31      <span style=color:#099>1102</span>
</span></span><span style=display:flex><span>          <span style=color:#099>9</span>       0.25      0.25      0.25       <span style=color:#099>829</span>
</span></span><span style=display:flex><span>         <span style=color:#099>10</span>       0.53      0.53      0.53      <span style=color:#099>1118</span>
</span></span><span style=display:flex><span>         <span style=color:#099>11</span>       0.24      0.27      0.26      <span style=color:#099>1126</span>
</span></span><span style=display:flex><span>         <span style=color:#099>12</span>       0.34      0.31      0.32       <span style=color:#099>679</span>
</span></span><span style=display:flex><span>         <span style=color:#099>13</span>       0.39      0.36      0.37       <span style=color:#099>285</span>
</span></span><span style=display:flex><span>         <span style=color:#099>14</span>       0.31      0.30      0.30       <span style=color:#099>822</span>
</span></span><span style=display:flex><span>         <span style=color:#099>15</span>       0.40      0.43      0.41      <span style=color:#099>2865</span>
</span></span><span style=display:flex><span>         <span style=color:#099>16</span>       0.31      0.31      0.31      <span style=color:#099>1326</span>
</span></span><span style=display:flex><span>         <span style=color:#099>17</span>       0.26      0.21      0.23       <span style=color:#099>506</span>
</span></span><span style=display:flex><span>         <span style=color:#099>18</span>       0.29      0.31      0.30       <span style=color:#099>413</span>
</span></span><span style=display:flex><span>         <span style=color:#099>19</span>       0.23      0.18      0.20       <span style=color:#099>829</span>
</span></span><span style=display:flex><span>         <span style=color:#099>20</span>       0.41      0.39      0.40      <span style=color:#099>1198</span>
</span></span><span style=display:flex><span>         <span style=color:#099>21</span>       0.47      0.45      0.46      <span style=color:#099>1345</span>
</span></span><span style=display:flex><span>         <span style=color:#099>22</span>       0.28      0.26      0.27       <span style=color:#099>409</span>
</span></span><span style=display:flex><span>         <span style=color:#099>23</span>       0.26      0.26      0.26       <span style=color:#099>344</span>
</span></span><span style=display:flex><span>         <span style=color:#099>24</span>       0.47      0.54      0.50      <span style=color:#099>3541</span>
</span></span><span style=display:flex><span>         <span style=color:#099>25</span>       0.42      0.37      0.40      <span style=color:#099>1271</span>
</span></span><span style=display:flex><span>         <span style=color:#099>26</span>       0.57      0.60      0.59      <span style=color:#099>1549</span>
</span></span><span style=display:flex><span>         <span style=color:#099>27</span>       0.18      0.14      0.16       <span style=color:#099>600</span>
</span></span><span style=display:flex><span>         <span style=color:#099>28</span>       0.51      0.47      0.49       <span style=color:#099>732</span>
</span></span><span style=display:flex><span>         <span style=color:#099>29</span>       0.31      0.31      0.31       <span style=color:#099>307</span>
</span></span><span style=display:flex><span>         <span style=color:#099>30</span>       0.39      0.34      0.37       <span style=color:#099>639</span>
</span></span><span style=display:flex><span>         <span style=color:#099>31</span>       0.30      0.30      0.30      <span style=color:#099>1264</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>avg / total       0.43      0.43      0.43     <span style=color:#099>42265</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>[[</span><span style=color:#099>1547</span>    <span style=color:#099>3</span>   <span style=color:#099>27</span> ...    <span style=color:#099>2</span>    <span style=color:#099>4</span>   23<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span>   <span style=color:#099>4</span>  <span style=color:#099>135</span>   <span style=color:#099>13</span> ...    <span style=color:#099>0</span>    <span style=color:#099>0</span>   54<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span>  <span style=color:#099>27</span>   <span style=color:#099>21</span>  <span style=color:#099>631</span> ...    <span style=color:#099>3</span>    <span style=color:#099>2</span>   32<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> ...
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span>   <span style=color:#099>4</span>    <span style=color:#099>2</span>    <span style=color:#099>5</span> ...   <span style=color:#099>94</span>    <span style=color:#099>3</span>    1<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span>   <span style=color:#099>3</span>    <span style=color:#099>7</span>    <span style=color:#099>1</span> ...    <span style=color:#099>2</span>  <span style=color:#099>220</span>   19<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span>  <span style=color:#099>28</span>   <span style=color:#099>62</span>   <span style=color:#099>35</span> ...    <span style=color:#099>0</span>   <span style=color:#099>19</span>  374<span style=color:#000;font-weight:700>]]</span>
</span></span></code></pre></div></div></article><button class=floating-button>
<a class=floating-button__link href=https://jdvala.github.io><span>home</span></a></button></div><footer class=post-footer><div class=footer><div>© 2020, Jay Vala. Theme - Origin by Andrey Parfenov</div><div class=footer__socials><a href=www.github.com/jdvala target=_blank class=social-link title="Github link" rel=noopener aria-label="follow on Github——Opens in a new window"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M0 0v24h24V0H0zm14.534 19.59c-.406.078-.534-.171-.534-.384v-2.195c0-.747-.262-1.233-.55-1.481 1.782-.198 3.654-.875 3.654-3.947.0-.874-.311-1.588-.824-2.147.083-.202.357-1.016-.079-2.117.0.0-.671-.215-2.198.82-.639-.18-1.323-.267-2.003-.271-.68.003-1.364.091-2.003.269-1.528-1.035-2.2-.82-2.2-.82-.434 1.102-.16 1.915-.077 2.118-.512.56-.824 1.273-.824 2.147.0 3.064 1.867 3.751 3.645 3.954-.229.2-.436.552-.508 1.07-.457.204-1.614.557-2.328-.666.0.0-.423-.768-1.227-.825.0.0-.78-.01-.055.487.0.0.525.246.889 1.17.0.0.463 1.428 2.688.944v1.489c0 .211-.129.459-.528.385C6.292 18.533 4 15.534 4 12c0-4.419 3.582-8 8-8s8 3.581 8 8c0 3.533-2.289 6.531-5.466 7.59z"/></svg></a></div></div></footer><script src=https://jdvala.github.io/js/script.js></script></body></html>