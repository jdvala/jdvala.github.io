<!doctype html><html lang=en><head><meta charset=utf-8><title>Jay Vala</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Training a Long Short Term Memory Network on subset of classes"><meta property="og:title" content="LSTM with 8 Classes"><meta property="og:description" content="Training a Long Short Term Memory Network on subset of classes"><meta property="og:type" content="website"><meta property="og:url" content="https://jdvala.github.io/posts/2018-05-25-rnn-sequence-classification-with-modifications/"><meta itemprop=name content="LSTM with 8 Classes"><meta itemprop=description content="Training a Long Short Term Memory Network on subset of classes"><meta name=twitter:card content="summary"><meta name=twitter:title content="LSTM with 8 Classes"><meta name=twitter:description content="Training a Long Short Term Memory Network on subset of classes"><link rel=apple-touch-icon sizes=180x180 href=apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=favicon-32.png><link rel=stylesheet href=https://jdvala.github.io/scss/style.min.d1aa507e320f63a9a89fb4d16c025955cea1564900de1060a4b2d7cabbabcdec.css></head><body><header><div class="header header-frame"><div><h1 class=header__title>LSTM with 8 Classes</h1><div class=header__description>Training a Long Short Term Memory Network on subset of classes</div></div><nav class=header-nav><ul class="header-nav-list header-nav-list--menu"><li class=header-nav-list__item><a class=header-nav-list__link href=/about/><span>About</span></a></li></ul><button class=header-nav-list__nav-btn>navigation</button></nav><button class=mb-header__menu-btn>
<span class=mb-header__menu-btn-line></span>
<span class=mb-header__menu-btn-line></span>
<span class=mb-header__menu-btn-line></span></button></div><nav id=mobile-header-nav class=mb-header-nav><button class="mb-header-nav__close-btn flex-center"><svg class="mb-header-nav__svg-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="32" height="32"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/><path d="M0 0h24v24H0z" fill="none"/></svg></button><div class=mb-header-nav__wrapper><div class=mb-header-nav__container><svg width="240" height="72" viewBox="0 0 240 72" class="mb-header-nav__title"><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle">Tags</text></svg><ul class=mb-header-nav-list><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/python/>python</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/text/>text</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/dataset/>dataset</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/nlp/>nlp</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/ai/>AI</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/rnn/>RNN</a></li></ul></div><div class=mb-header-nav__container><svg width="240" height="72" viewBox="0 0 240 72" class="mb-header-nav__title"><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle">Menu</text></svg><ul class=mb-header-nav-list><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=/about/>About</a></li></ul></div></div></nav></header><div id=content><article class=post><div class=post-content><h1 id=new-lstm-with-8-classes>New LSTM with 8 classes</h1><p>In this script I will be using only 8 out of 32 classes that were originaly present in the dataset, this is necessary because the data in other classes is much less compared to these 8 classes, this makes it difficult for the neural network to learn anything off of those classes.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>pickle</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>gensim.models</span> <span style=color:#000;font-weight:700>import</span> Word2Vec
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>keras</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>numpy</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>np</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>pandas</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>pd</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>keras.datasets</span> <span style=color:#000;font-weight:700>import</span> imdb
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>keras.models</span> <span style=color:#000;font-weight:700>import</span> Sequential
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>keras.layers</span> <span style=color:#000;font-weight:700>import</span> Dense, Dropout
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>keras.layers</span> <span style=color:#000;font-weight:700>import</span> LSTM
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>keras.layers.embeddings</span> <span style=color:#000;font-weight:700>import</span> Embedding
</span></span></code></pre></div><pre><code>/home/jay/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
</code></pre><p>I have already stored the senteces and their corresponding labels as stated in this <a href=https://jdvala.github.io/blog.io/thesis/2018/05/23/Creating-Data-Set-Again-!.html>post</a>. Hence I am going to use those sentences and labels.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Loading data</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>with</span> <span style=color:#0086b3>open</span>(<span style=color:#d14>&#39;/home/jay/pickled/sent.pkl&#39;</span>, <span style=color:#d14>&#39;rb&#39;</span>) <span style=color:#000;font-weight:700>as</span> f:
</span></span><span style=display:flex><span>    sents <span style=color:#000;font-weight:700>=</span> pickle<span style=color:#000;font-weight:700>.</span>load(f)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>with</span> <span style=color:#0086b3>open</span>(<span style=color:#d14>&#39;/home/jay/pickled/label.pkl&#39;</span>, <span style=color:#d14>&#39;rb&#39;</span>) <span style=color:#000;font-weight:700>as</span> g:
</span></span><span style=display:flex><span>    label <span style=color:#000;font-weight:700>=</span> pickle<span style=color:#000;font-weight:700>.</span>load(g)
</span></span></code></pre></div><p>As the model was not learning, I have decided to reduce the number of classes to 8, so I will pick up top 8 classes with maximum number of samples.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># lets count number of sample in each class</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>collections</span> <span style=color:#000;font-weight:700>import</span> Counter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>counter <span style=color:#000;font-weight:700>=</span> Counter()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>for</span> l <span style=color:#000;font-weight:700>in</span> label:
</span></span><span style=display:flex><span>    counter[l] <span style=color:#000;font-weight:700>+=</span><span style=color:#099>1</span>
</span></span></code></pre></div><p>As I have counted number of samples in each class, I will pick out top 8 classes from them and use only the 8</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>import</span> <span style=color:#555>heapq</span>
</span></span><span style=display:flex><span>classes <span style=color:#000;font-weight:700>=</span> heapq<span style=color:#000;font-weight:700>.</span>nlargest(<span style=color:#099>8</span>,<span style=color:#0086b3>zip</span>(counter<span style=color:#000;font-weight:700>.</span>values(), counter<span style=color:#000;font-weight:700>.</span>keys()))
</span></span><span style=display:flex><span>classes
</span></span></code></pre></div><pre><code>[(19018, 4),
 (15679, 24),
 (12455, 15),
 (10219, 3),
 (10011, 0),
 (7008, 26),
 (6606, 5),
 (6392, 11)]
</code></pre><p>These are the classes I am going to use for training the neural network.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sample_8 <span style=color:#000;font-weight:700>=</span>[]
</span></span><span style=display:flex><span>label_8 <span style=color:#000;font-weight:700>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>for</span> label, sent <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>zip</span>(label, sents):
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>if</span> (label <span style=color:#000;font-weight:700>==</span> <span style=color:#099>4</span>) <span style=color:#000;font-weight:700>or</span> (label <span style=color:#000;font-weight:700>==</span><span style=color:#099>24</span>) <span style=color:#000;font-weight:700>or</span> (label <span style=color:#000;font-weight:700>==</span> <span style=color:#099>15</span>) <span style=color:#000;font-weight:700>or</span> (label<span style=color:#000;font-weight:700>==</span><span style=color:#099>3</span>) <span style=color:#000;font-weight:700>or</span> (label<span style=color:#000;font-weight:700>==</span><span style=color:#099>0</span>) <span style=color:#000;font-weight:700>or</span> (label<span style=color:#000;font-weight:700>==</span><span style=color:#099>26</span>) <span style=color:#000;font-weight:700>or</span> (label<span style=color:#000;font-weight:700>==</span><span style=color:#099>5</span>) <span style=color:#000;font-weight:700>or</span> (label<span style=color:#000;font-weight:700>==</span><span style=color:#099>11</span>):
</span></span><span style=display:flex><span>        sample_8<span style=color:#000;font-weight:700>.</span>append(sent)
</span></span><span style=display:flex><span>        label_8<span style=color:#000;font-weight:700>.</span>append(label)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#0086b3>len</span>(sample_8)
</span></span></code></pre></div><pre><code>87388
</code></pre><p>As I have stated in my <a href=https://jdvala.github.io/blog.io/thesis/2018/05/16/Data-Discrepancies.html>Data Descrepancies post</a> that some sentences are very long. This is a problem when traning recurrent neural network with long sequences.
For example think of a situation where you have a sentence of 10 words and another sentence which is 200 word sentence which also happens to be the maximum length of sentence in dataset. So when you are padding all the sentences the sentence with 10 words will have 190 padded zeros, so you see how this is very inefficient and that there has to be some solution to things like this.
To tackle this problem what I think is a easy and effective solution is to break the 200 word sentence into number of smaller sentneces. This is however not ideal as we are loosing semantics and realations between words but this seams to be the only viable option right now. There are other options which you can learn about it <a href=https://machinelearningmastery.com/handle-long-sequences-long-short-term-memory-recurrent-neural-networks/>here</a>.</p><p>So for me what I will do is I will use sliding window or rolling window algorithm. To preserve the context of words in the longer sequence I will take smaller steps. For example, I have a sentence of 100 words, so I will first take a window of 20 words and make it into a sentence, then I will move five words from the starting and take another 20 words. So in my first sentence I will have words from index <code>0-19</code> and in my second sentence i will have words from index <code>4-24</code> and so on. This helps in preserving the context of the sentences.</p><p>The setting I used is <code>winSize=20</code> and <code>step=10</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>slidingWindow</span>(sequence,winSize,step):
</span></span><span style=display:flex><span>    <span style=color:#d14>&#34;&#34;&#34;Returns a generator that will iterate through
</span></span></span><span style=display:flex><span><span style=color:#d14>       the defined chunks of input sequence. Input sequence
</span></span></span><span style=display:flex><span><span style=color:#d14>       must be sliceable.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Pre-compute number of chunks to emit</span>
</span></span><span style=display:flex><span>    numOfChunks <span style=color:#000;font-weight:700>=</span> <span style=color:#0086b3>round</span>(((<span style=color:#0086b3>len</span>(sequence)<span style=color:#000;font-weight:700>-</span>winSize)<span style=color:#000;font-weight:700>/</span>step)<span style=color:#000;font-weight:700>+</span><span style=color:#099>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Do the work</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>for</span> i <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(<span style=color:#099>0</span>,numOfChunks<span style=color:#000;font-weight:700>*</span>step,step):
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>yield</span> sequence[i:i<span style=color:#000;font-weight:700>+</span>winSize]
</span></span></code></pre></div><p>Make new sentece and labels list and check the length of every sentnce in <code>sample_8</code>, if there are more than 20 words in the sentence apply sliding window on it and store them into new list</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>new_sent <span style=color:#000;font-weight:700>=</span> []
</span></span><span style=display:flex><span>new_label <span style=color:#000;font-weight:700>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>for</span> s, l <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>zip</span>(sample_8, label_8):
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>if</span> (<span style=color:#0086b3>len</span>(s<span style=color:#000;font-weight:700>.</span>split()))<span style=color:#000;font-weight:700>&lt;=</span><span style=color:#099>20</span>:
</span></span><span style=display:flex><span>        new_sent<span style=color:#000;font-weight:700>.</span>append(s)
</span></span><span style=display:flex><span>        new_label<span style=color:#000;font-weight:700>.</span>append(l)
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>if</span> (<span style=color:#0086b3>len</span>(s<span style=color:#000;font-weight:700>.</span>split()))<span style=color:#000;font-weight:700>&gt;</span><span style=color:#099>20</span>:
</span></span><span style=display:flex><span>        slides <span style=color:#000;font-weight:700>=</span> slidingWindow(s, <span style=color:#099>20</span>, <span style=color:#099>10</span>)
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>for</span> slide <span style=color:#000;font-weight:700>in</span> slides:
</span></span><span style=display:flex><span>            new_sent<span style=color:#000;font-weight:700>.</span>append(<span style=color:#d14>&#39; &#39;</span><span style=color:#000;font-weight:700>.</span>join(slide))
</span></span><span style=display:flex><span>            new_label<span style=color:#000;font-weight:700>.</span>append(l)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#0086b3>len</span>(new_sent)
</span></span></code></pre></div><pre><code>329068
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Defining max_length</span>
</span></span><span style=display:flex><span>max_sent <span style=color:#000;font-weight:700>=</span> <span style=color:#0086b3>max</span>(new_sent, key<span style=color:#000;font-weight:700>=</span><span style=color:#0086b3>len</span>)    <span style=color:#998;font-style:italic># Get the longest sentence in the list</span>
</span></span><span style=display:flex><span>max_length <span style=color:#000;font-weight:700>=</span> <span style=color:#0086b3>len</span>(max_sent<span style=color:#000;font-weight:700>.</span>split())  <span style=color:#998;font-style:italic># split it and set the max_length</span>
</span></span><span style=display:flex><span>max_length
</span></span></code></pre></div><pre><code>17
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Using Keras tokenizer</span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># Here I am using 10000 words to keep based on the word frequency</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#000;font-weight:700>=</span> keras<span style=color:#000;font-weight:700>.</span>preprocessing<span style=color:#000;font-weight:700>.</span>text<span style=color:#000;font-weight:700>.</span>Tokenizer(num_words<span style=color:#000;font-weight:700>=</span><span style=color:#099>10000</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># Tokenizing the sentences (This process may take some time depending on your corpus size)</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#000;font-weight:700>.</span>fit_on_texts(new_sent)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Lets see what our vocabulary size is</span>
</span></span><span style=display:flex><span>vocab_size <span style=color:#000;font-weight:700>=</span> <span style=color:#0086b3>len</span>(tokenizer<span style=color:#000;font-weight:700>.</span>word_index) <span style=color:#000;font-weight:700>+</span> <span style=color:#099>1</span>   <span style=color:#998;font-style:italic># We are adding 1 here because it takes indexing from zero</span>
</span></span><span style=display:flex><span>vocab_size
</span></span></code></pre></div><pre><code>15532
</code></pre><p>Even though it shows the vocab size to be 15532 it will only use 10000 words as we have spacified above.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Sentence encoding</span>
</span></span><span style=display:flex><span>sent_encoded <span style=color:#000;font-weight:700>=</span> tokenizer<span style=color:#000;font-weight:700>.</span>texts_to_sequences(new_sent)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Start padding with the max_length</span>
</span></span><span style=display:flex><span>padded_sents <span style=color:#000;font-weight:700>=</span> keras<span style=color:#000;font-weight:700>.</span>preprocessing<span style=color:#000;font-weight:700>.</span>sequence<span style=color:#000;font-weight:700>.</span>pad_sequences(sent_encoded, maxlen<span style=color:#000;font-weight:700>=</span>max_length, padding<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;post&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># Note: We are using post padding</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Split the data into test and train</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>sklearn.model_selection</span> <span style=color:#000;font-weight:700>import</span> train_test_split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_set, test_set, train_label, test_label <span style=color:#000;font-weight:700>=</span> train_test_split(padded_sents, label_8,test_size<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.33</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>sklearn.preprocessing</span> <span style=color:#000;font-weight:700>import</span> OneHotEncoder
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># Define one_hot_encoder object</span>
</span></span><span style=display:flex><span>onehot_encoder <span style=color:#000;font-weight:700>=</span> OneHotEncoder(sparse<span style=color:#000;font-weight:700>=</span><span style=color:#000;font-weight:700>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_labels <span style=color:#000;font-weight:700>=</span> onehot_encoder<span style=color:#000;font-weight:700>.</span>fit_transform(np<span style=color:#000;font-weight:700>.</span>reshape(train_label,(<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>,<span style=color:#099>1</span>)))
</span></span><span style=display:flex><span>test_labels <span style=color:#000;font-weight:700>=</span> onehot_encoder<span style=color:#000;font-weight:700>.</span>fit_transform(np<span style=color:#000;font-weight:700>.</span>reshape(test_label,(<span style=color:#000;font-weight:700>-</span><span style=color:#099>1</span>,<span style=color:#099>1</span>)))
</span></span></code></pre></div><p>As I know that classes are unbalanced, the most common parctice is to use class weights to award penalty to a wrong prediction. For example, if we have 2 classes with <code>class_1 = 80</code> and <code>class_2 = 20</code> samples, then we can award a penalty to our network if the sample being predicted is of <code>class_2</code> and is predicted to <code>class_1</code>. To calculate penalty we take a reference class, I usually take the class with maximum samples, and then calculate penalty for other classes with this reference class. The penalty for <code>class_1 = (no. of sample in reference class)/(no. of sample in class_1)</code> which for our example will be <code>80/80=1</code> and for <code>class_2 = 80/20 = 4.0</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Computing the class weights</span>
</span></span><span style=display:flex><span>class_ <span style=color:#000;font-weight:700>=</span> []
</span></span><span style=display:flex><span>weights <span style=color:#000;font-weight:700>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ref_class <span style=color:#000;font-weight:700>=</span> classes[<span style=color:#099>0</span>][<span style=color:#099>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>for</span> i, j <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>enumerate</span>(classes):
</span></span><span style=display:flex><span>    class_<span style=color:#000;font-weight:700>.</span>append(i)
</span></span><span style=display:flex><span>    weights<span style=color:#000;font-weight:700>.</span>append(ref_class<span style=color:#000;font-weight:700>/</span>j[<span style=color:#099>0</span>])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Creating dictonary out of the two lists</span>
</span></span><span style=display:flex><span>class_weight <span style=color:#000;font-weight:700>=</span> <span style=color:#0086b3>dict</span>(<span style=color:#0086b3>zip</span>(class_, weights))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>class_weight
</span></span></code></pre></div><pre><code>{0: 1.0,
 1: 1.2129600102047324,
 2: 1.5269369731031714,
 3: 1.8610431549075253,
 4: 1.8997103186494855,
 5: 2.713755707762557,
 6: 2.8788979715410234,
 7: 2.9752816020025032}
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Create sequential model</span>
</span></span><span style=display:flex><span>model <span style=color:#000;font-weight:700>=</span> Sequential()
</span></span><span style=display:flex><span>reduce_rate <span style=color:#000;font-weight:700>=</span> keras<span style=color:#000;font-weight:700>.</span>callbacks<span style=color:#000;font-weight:700>.</span>ReduceLROnPlateau(monitor<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;val_loss&#39;</span>, factor<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.1</span>, patience<span style=color:#000;font-weight:700>=</span><span style=color:#099>0</span>, verbose<span style=color:#000;font-weight:700>=</span><span style=color:#099>1</span>, 
</span></span><span style=display:flex><span>                                                mode<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;auto&#39;</span>, epsilon<span style=color:#000;font-weight:700>=</span><span style=color:#099>0.0001</span>, cooldown<span style=color:#000;font-weight:700>=</span><span style=color:#099>0</span>, min_lr<span style=color:#000;font-weight:700>=</span><span style=color:#099>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>early_stop <span style=color:#000;font-weight:700>=</span> keras<span style=color:#000;font-weight:700>.</span>callbacks<span style=color:#000;font-weight:700>.</span>EarlyStopping(monitor<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;val_loss&#39;</span>, min_delta<span style=color:#000;font-weight:700>=</span><span style=color:#099>0</span>, 
</span></span><span style=display:flex><span>                                           patience<span style=color:#000;font-weight:700>=</span><span style=color:#099>1</span>, verbose<span style=color:#000;font-weight:700>=</span><span style=color:#099>1</span>, mode<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;auto&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>add(Embedding(vocab_size, <span style=color:#099>1000</span>, input_length<span style=color:#000;font-weight:700>=</span>max_length))   <span style=color:#998;font-style:italic># adding embedding layer, which we have defined earlier</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>add(LSTM(<span style=color:#099>30</span>,recurrent_regularizer<span style=color:#000;font-weight:700>=</span>keras<span style=color:#000;font-weight:700>.</span>regularizers<span style=color:#000;font-weight:700>.</span>l2(<span style=color:#099>0.1</span>),return_sequences<span style=color:#000;font-weight:700>=</span><span style=color:#000;font-weight:700>True</span>))     <span style=color:#998;font-style:italic># LSTM layer </span>
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>add(Dropout(<span style=color:#099>0.1</span>))
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>add(LSTM(<span style=color:#099>50</span>,recurrent_regularizer<span style=color:#000;font-weight:700>=</span>keras<span style=color:#000;font-weight:700>.</span>regularizers<span style=color:#000;font-weight:700>.</span>l2(<span style=color:#099>0.06</span>),return_sequences<span style=color:#000;font-weight:700>=</span><span style=color:#000;font-weight:700>False</span>))
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic>#model.add(Dropout(0.5))</span>
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>add(Dense(<span style=color:#099>8</span>, activation<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;softmax&#39;</span>))
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>compile(loss<span style=color:#000;font-weight:700>=</span><span style=color:#d14>&#39;categorical_crossentropy&#39;</span>, optimizer<span style=color:#000;font-weight:700>=</span> <span style=color:#d14>&#39;adam&#39;</span>, metrics<span style=color:#000;font-weight:700>=</span>[<span style=color:#d14>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(model<span style=color:#000;font-weight:700>.</span>summary())
</span></span><span style=display:flex><span>model<span style=color:#000;font-weight:700>.</span>fit(train_set, train_labels, validation_split<span style=color:#000;font-weight:700>=</span><span style=color:#099>.1</span>, epochs<span style=color:#000;font-weight:700>=</span><span style=color:#099>10</span>, batch_size<span style=color:#000;font-weight:700>=</span><span style=color:#099>512</span>, 
</span></span><span style=display:flex><span>          verbose<span style=color:#000;font-weight:700>=</span><span style=color:#099>1</span>,class_weight<span style=color:#000;font-weight:700>=</span>class_weight ,callbacks<span style=color:#000;font-weight:700>=</span>[reduce_rate,early_stop])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>/home/jay/.local/lib/python3.5/site-packages/keras/callbacks.py:928: UserWarning: <span style=color:#d14>`</span>epsilon<span style=color:#d14>`</span> argument is deprecated and will be removed, use <span style=color:#d14>`</span>min_delta<span style=color:#d14>`</span> insted.
</span></span><span style=display:flex><span>  warnings.warn<span style=color:#000;font-weight:700>(</span><span style=color:#d14>&#39;`epsilon` argument is deprecated and &#39;</span>
</span></span><span style=display:flex><span>_________________________________________________________________
</span></span><span style=display:flex><span>Layer <span style=color:#000;font-weight:700>(</span><span style=color:#0086b3>type</span><span style=color:#000;font-weight:700>)</span>                 Output Shape              Param <span style=color:#998;font-style:italic>#   </span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>=================================================================</span>
</span></span><span style=display:flex><span>embedding_1 <span style=color:#000;font-weight:700>(</span>Embedding<span style=color:#000;font-weight:700>)</span>      <span style=color:#000;font-weight:700>(</span>None, 20, 1000<span style=color:#000;font-weight:700>)</span>          <span style=color:#099>15356000</span>  
</span></span><span style=display:flex><span>_________________________________________________________________
</span></span><span style=display:flex><span>lstm_1 <span style=color:#000;font-weight:700>(</span>LSTM<span style=color:#000;font-weight:700>)</span>                <span style=color:#000;font-weight:700>(</span>None, 20, 30<span style=color:#000;font-weight:700>)</span>            <span style=color:#099>123720</span>    
</span></span><span style=display:flex><span>_________________________________________________________________
</span></span><span style=display:flex><span>dropout_1 <span style=color:#000;font-weight:700>(</span>Dropout<span style=color:#000;font-weight:700>)</span>          <span style=color:#000;font-weight:700>(</span>None, 20, 30<span style=color:#000;font-weight:700>)</span>            <span style=color:#099>0</span>         
</span></span><span style=display:flex><span>_________________________________________________________________
</span></span><span style=display:flex><span>lstm_2 <span style=color:#000;font-weight:700>(</span>LSTM<span style=color:#000;font-weight:700>)</span>                <span style=color:#000;font-weight:700>(</span>None, 50<span style=color:#000;font-weight:700>)</span>                <span style=color:#099>16200</span>     
</span></span><span style=display:flex><span>_________________________________________________________________
</span></span><span style=display:flex><span>dense_1 <span style=color:#000;font-weight:700>(</span>Dense<span style=color:#000;font-weight:700>)</span>              <span style=color:#000;font-weight:700>(</span>None, 8<span style=color:#000;font-weight:700>)</span>                 <span style=color:teal>408</span>       
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>=================================================================</span>
</span></span><span style=display:flex><span>Total params: 15,496,328
</span></span><span style=display:flex><span>Trainable params: 15,496,328
</span></span><span style=display:flex><span>Non-trainable params: <span style=color:#099>0</span>
</span></span><span style=display:flex><span>Train on <span style=color:#099>68274</span> samples, validate on <span style=color:#099>7587</span> samples
</span></span><span style=display:flex><span>Epoch 1/10
</span></span><span style=display:flex><span>68274/68274 <span style=color:#000;font-weight:700>[==============================]</span> - 16s 227us/step - loss: 5.3191 - acc: 0.3646 - val_loss: 2.8831 - val_acc: 0.5347
</span></span><span style=display:flex><span>Epoch 2/10
</span></span><span style=display:flex><span>68274/68274 <span style=color:#000;font-weight:700>[==============================]</span> - 13s 192us/step - loss: 2.3372 - acc: 0.5942 - val_loss: 2.1347 - val_acc: 0.6038
</span></span><span style=display:flex><span>Epoch 3/10
</span></span><span style=display:flex><span>68274/68274 <span style=color:#000;font-weight:700>[==============================]</span> - 13s 193us/step - loss: 1.8782 - acc: 0.6433 - val_loss: 2.0397 - val_acc: 0.6060
</span></span><span style=display:flex><span>Epoch 4/10
</span></span><span style=display:flex><span>68274/68274 <span style=color:#000;font-weight:700>[==============================]</span> - 13s 194us/step - loss: 1.6965 - acc: 0.6656 - val_loss: 2.0175 - val_acc: 0.6137
</span></span><span style=display:flex><span>Epoch 5/10
</span></span><span style=display:flex><span>68274/68274 <span style=color:#000;font-weight:700>[==============================]</span> - 13s 193us/step - loss: 1.5890 - acc: 0.6803 - val_loss: 2.0452 - val_acc: 0.6171
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
</span></span><span style=display:flex><span>Epoch 00005: early stopping
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>scores <span style=color:#000;font-weight:700>=</span> model<span style=color:#000;font-weight:700>.</span>evaluate(test_set, test_labels, verbose<span style=color:#000;font-weight:700>=</span><span style=color:#099>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(<span style=color:#d14>&#34;Accuracy: </span><span style=color:#d14>%.2f%%</span><span style=color:#d14>&#34;</span> <span style=color:#000;font-weight:700>%</span> (scores[<span style=color:#099>1</span>]<span style=color:#000;font-weight:700>*</span><span style=color:#099>100</span>))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>37365/37365 <span style=color:#000;font-weight:700>[==============================]</span> - 23s 622us/step
</span></span><span style=display:flex><span>Accuracy: 61.45%
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Classification Report (Precision, Recall and F1-Score)</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>from</span> <span style=color:#555>sklearn.metrics</span> <span style=color:#000;font-weight:700>import</span> classification_report, confusion_matrix
</span></span><span style=display:flex><span>classificationReport <span style=color:#000;font-weight:700>=</span> classification_report(y_true, y_pred)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(classificationReport)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#0086b3>print</span>(confusion_matrix(y_true, y_pred))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>precision    recall  f1-score   support
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>          <span style=color:#099>0</span>       0.73      0.47      0.57      <span style=color:#099>4268</span>
</span></span><span style=display:flex><span>          <span style=color:#099>1</span>       0.67      0.45      0.54      <span style=color:#099>4169</span>
</span></span><span style=display:flex><span>          <span style=color:#099>2</span>       0.79      0.67      0.73      <span style=color:#099>8563</span>
</span></span><span style=display:flex><span>          <span style=color:#099>3</span>       0.60      0.52      0.56      <span style=color:#099>2816</span>
</span></span><span style=display:flex><span>          <span style=color:#099>4</span>       0.52      0.46      0.49      <span style=color:#099>2400</span>
</span></span><span style=display:flex><span>          <span style=color:#099>5</span>       0.62      0.67      0.64      <span style=color:#099>5409</span>
</span></span><span style=display:flex><span>          <span style=color:#099>6</span>       0.46      0.75      0.57      <span style=color:#099>6799</span>
</span></span><span style=display:flex><span>          <span style=color:#099>7</span>       0.66      0.68      0.67      <span style=color:#099>2941</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>avg / total       0.64      0.61      0.62     <span style=color:#099>37365</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>[[</span><span style=color:#099>1989</span>  <span style=color:#099>106</span>  <span style=color:#099>147</span>   <span style=color:#099>80</span>  <span style=color:#099>245</span>  <span style=color:#099>769</span>  <span style=color:#099>890</span>   42<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span>  <span style=color:#099>90</span> <span style=color:#099>1886</span>  <span style=color:#099>370</span>  <span style=color:#099>130</span>  <span style=color:#099>287</span>  <span style=color:#099>230</span> <span style=color:#099>1039</span>  137<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span>  <span style=color:#099>58</span>  <span style=color:#099>215</span> <span style=color:#099>5753</span>  <span style=color:#099>380</span>  <span style=color:#099>127</span>  <span style=color:#099>297</span> <span style=color:#099>1506</span>  227<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span>  <span style=color:#099>60</span>   <span style=color:#099>95</span>  <span style=color:#099>376</span> <span style=color:#099>1469</span>   <span style=color:#099>52</span>  <span style=color:#099>155</span>  <span style=color:#099>537</span>   72<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span> <span style=color:#099>114</span>  <span style=color:#099>185</span>  <span style=color:#099>101</span>   <span style=color:#099>39</span> <span style=color:#099>1108</span>  <span style=color:#099>316</span>  <span style=color:#099>506</span>   31<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span> <span style=color:#099>309</span>   <span style=color:#099>49</span>  <span style=color:#099>142</span>  <span style=color:#099>102</span>  <span style=color:#099>206</span> <span style=color:#099>3635</span>  <span style=color:#099>882</span>   84<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span>  <span style=color:#099>87</span>  <span style=color:#099>239</span>  <span style=color:#099>272</span>  <span style=color:#099>168</span>   <span style=color:#099>99</span>  <span style=color:#099>399</span> <span style=color:#099>5114</span>  421<span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span> <span style=color:#000;font-weight:700>[</span>   <span style=color:#099>3</span>   <span style=color:#099>21</span>   <span style=color:#099>99</span>   <span style=color:#099>66</span>   <span style=color:#099>22</span>   <span style=color:#099>78</span>  <span style=color:#099>645</span> 2007<span style=color:#000;font-weight:700>]]</span>
</span></span></code></pre></div></div></article><button class=floating-button>
<a class=floating-button__link href=https://jdvala.github.io><span>home</span></a></button></div><footer class=post-footer><div class=footer><div>© 2020, Jay Vala. Theme - Origin by Andrey Parfenov</div><div class=footer__socials><a href=www.github.com/jdvala target=_blank class=social-link title="Github link" rel=noopener aria-label="follow on Github——Opens in a new window"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M0 0v24h24V0H0zm14.534 19.59c-.406.078-.534-.171-.534-.384v-2.195c0-.747-.262-1.233-.55-1.481 1.782-.198 3.654-.875 3.654-3.947.0-.874-.311-1.588-.824-2.147.083-.202.357-1.016-.079-2.117.0.0-.671-.215-2.198.82-.639-.18-1.323-.267-2.003-.271-.68.003-1.364.091-2.003.269-1.528-1.035-2.2-.82-2.2-.82-.434 1.102-.16 1.915-.077 2.118-.512.56-.824 1.273-.824 2.147.0 3.064 1.867 3.751 3.645 3.954-.229.2-.436.552-.508 1.07-.457.204-1.614.557-2.328-.666.0.0-.423-.768-1.227-.825.0.0-.78-.01-.055.487.0.0.525.246.889 1.17.0.0.463 1.428 2.688.944v1.489c0 .211-.129.459-.528.385C6.292 18.533 4 15.534 4 12c0-4.419 3.582-8 8-8s8 3.581 8 8c0 3.533-2.289 6.531-5.466 7.59z"/></svg></a></div></div></footer><script src=https://jdvala.github.io/js/script.js></script></body></html>