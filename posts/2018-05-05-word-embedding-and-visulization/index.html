<!doctype html><html lang=en><head><meta charset=utf-8><title>Jay Vala</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Visualization of word embedding and how to use pretrained word embeddings"><meta property="og:title" content="Word Embedding and It's visualization"><meta property="og:description" content="Visualization of word embedding and how to use pretrained word embeddings"><meta property="og:type" content="website"><meta property="og:url" content="https://jdvala.github.io/posts/2018-05-05-word-embedding-and-visulization/"><meta itemprop=name content="Word Embedding and It's visualization"><meta itemprop=description content="Visualization of word embedding and how to use pretrained word embeddings"><meta name=twitter:card content="summary"><meta name=twitter:title content="Word Embedding and It's visualization"><meta name=twitter:description content="Visualization of word embedding and how to use pretrained word embeddings"><link rel=apple-touch-icon sizes=180x180 href=apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=favicon-32.png><link rel=stylesheet href=https://jdvala.github.io/scss/style.min.d1aa507e320f63a9a89fb4d16c025955cea1564900de1060a4b2d7cabbabcdec.css></head><body><header><div class="header header-frame"><div><h1 class=header__title>Word Embedding and It's visualization</h1><div class=header__description>Visualization of word embedding and how to use pretrained word embeddings</div></div><nav class=header-nav><ul class="header-nav-list header-nav-list--menu"><li class=header-nav-list__item><a class=header-nav-list__link href=/about/><span>About</span></a></li></ul><button class=header-nav-list__nav-btn>navigation</button></nav><button class=mb-header__menu-btn>
<span class=mb-header__menu-btn-line></span><span class=mb-header__menu-btn-line></span><span class=mb-header__menu-btn-line></span></button></div><nav id=mobile-header-nav class=mb-header-nav><button class="mb-header-nav__close-btn flex-center"><svg class="mb-header-nav__svg-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="32" height="32"
            ><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"
                /><path d="M0 0h24v24H0z" fill="none" /></svg></button><div class=mb-header-nav__wrapper><div class=mb-header-nav__container><svg width="240" height="72" viewBox="0 0 240 72" class="mb-header-nav__title"
                ><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle">Tags</text></svg><ul class=mb-header-nav-list><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/python/>python</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/nlp/>nlp</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/word2vec/>word2vec</a></li><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=https://jdvala.github.io/tags/text/>text</a></li></ul></div><div class=mb-header-nav__container><svg width="240" height="72" viewBox="0 0 240 72" class="mb-header-nav__title"
                ><text x="50%" y="50%" dominant-baseline="middle" text-anchor="middle">Menu</text></svg><ul class=mb-header-nav-list><li class=mb-header-nav-list__item><a class=mb-header-nav-list__link href=/about/>About</a></li></ul></div></div></nav></header><div id=content><article class=post><div class=post-content><h1 id=word-embeddings-and-its-visualizations>Word Embeddings and It&rsquo;s visualizations</h1><h2 id=creating-word-vectors>Creating Word Vectors</h2><p>+++
In the last post I have obtained perfect text out of the EU summaries, now the goal is to create word embeddings out of it and visualizing them in tensorboard projector. I am visualizing it right now because I want to know how to do it because I will need to visualize bilingual word embeddings when I create one to see how well the bilingual word embeddings are so that I can fine tune the process to fit best for my case.</p><p>For creating word embeddings I will use <a href=https://radimrehurek.com/gensim/>Gensim&rsquo;s</a> <a href=https://radimrehurek.com/gensim/models/word2vec.html>word2vec</a> model. So, In order to start I first need to see what <em>gensim</em> takes input as. It so happens that it takes words tokens to train word2vec. Hmm, I did not account for that in my previous post and I simply created sentence tokens, well first I need to create word tokens out of the sentences.</p><p>I wrote this python code to do the word tokens while preserving the sentences.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#000;font-weight:700>import</span> <span style=color:#555>os</span>

load_text <span style=color:#000;font-weight:700>=</span>[]  <span style=color:#998;font-style:italic>#List to store the loaded text</span>
<span style=color:#000;font-weight:700>for</span> root, dirs, files <span style=color:#000;font-weight:700>in</span> os<span style=color:#000;font-weight:700>.</span>walk(<span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>/home/jay/Ready</span><span style=color:#d14>&#34;</span>):
    <span style=color:#000;font-weight:700>for</span> <span style=color:#0086b3>file</span> <span style=color:#000;font-weight:700>in</span> files:
        <span style=color:#000;font-weight:700>if</span> <span style=color:#0086b3>file</span><span style=color:#000;font-weight:700>.</span>endswith(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>.txt</span><span style=color:#d14>&#39;</span>):
            <span style=color:#000;font-weight:700>with</span> <span style=color:#0086b3>open</span>(os<span style=color:#000;font-weight:700>.</span>path<span style=color:#000;font-weight:700>.</span>join(root, <span style=color:#0086b3>file</span>), <span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>r</span><span style=color:#d14>&#39;</span>) <span style=color:#000;font-weight:700>as</span> f:
                contents <span style=color:#000;font-weight:700>=</span> f<span style=color:#000;font-weight:700>.</span>read()
                contents <span style=color:#000;font-weight:700>=</span> re<span style=color:#000;font-weight:700>.</span>sub(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>summary</span><span style=color:#d14>&#39;</span>,<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>&#39;</span>,contents,flags<span style=color:#000;font-weight:700>=</span>re<span style=color:#000;font-weight:700>.</span>IGNORECASE)    <span style=color:#998;font-style:italic># Removing the word summary</span>
                words <span style=color:#000;font-weight:700>=</span> word_tokenize(contents)
                load_text<span style=color:#000;font-weight:700>.</span>append(words)
</code></pre></div><p>Now, that I have the data in the format I needed, I will run the word2vec model and create the word embedding</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#000;font-weight:700>import</span> <span style=color:#555>gensim</span>
<span style=color:#000;font-weight:700>import</span> <span style=color:#555>logging</span>

<span style=color:#998;font-style:italic># Here I will be using logging module of python to help me see the progress and statistics.</span>

<span style=color:#998;font-style:italic># Let&#39;s configure the logging module </span>
logging<span style=color:#000;font-weight:700>.</span>basicConfig(format<span style=color:#000;font-weight:700>=</span><span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>%(levelname)s</span><span style=color:#d14> : </span><span style=color:#d14>%(message)s</span><span style=color:#d14>&#39;</span>, level<span style=color:#000;font-weight:700>=</span> logging<span style=color:#000;font-weight:700>.</span>INFO)
</code></pre></div><p>It is important to set the logging level to <code>logging.INFO</code> so as to see all the details.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># Now lets declare the embedding size</span>
embedding_size <span style=color:#000;font-weight:700>=</span> <span style=color:#099>4000</span> 
</code></pre></div><p>Embedding size here means what is the maximum number of features one want to incorporate. So, here as I don&rsquo;t know what is the best one I selected the size 4000 to account for all the different words it has in the text(I wanted to go for more but my system would not allow it).</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># Now lets build the model and run it</span>
word_model <span style=color:#000;font-weight:700>=</span> gensim<span style=color:#000;font-weight:700>.</span>models<span style=color:#000;font-weight:700>.</span>Word2Vec(load_text, size<span style=color:#000;font-weight:700>=</span>embedding_size, min_count<span style=color:#000;font-weight:700>=</span><span style=color:#099>5</span>)
</code></pre></div><p>There are a lot of options that you can play with in the function but I am satisfied with these. However, there is an option <code>iter (int)</code> if you want to run it for more epochs(default = 5). The <code>min_count</code> argument is to say ignore all the words with total frequency lower than this.</p><p>The output will look something like this</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>NFO : collecting all words and their counts
INFO : PROGRESS: at sentence <span style=color:#998;font-style:italic>#0, processed 0 words, keeping 0 word types</span>
INFO : collected <span style=color:#099>16385</span> word types from a corpus of <span style=color:#099>1944658</span> raw words and <span style=color:#099>5293</span> sentences
INFO : Loading a fresh vocabulary
INFO : <span style=color:teal>min_count</span><span style=color:#000;font-weight:700>=</span><span style=color:#099>10</span> retains <span style=color:#099>6442</span> unique words <span style=color:#000;font-weight:700>(</span>39% of original 16385, drops 9943<span style=color:#000;font-weight:700>)</span>
INFO : <span style=color:teal>min_count</span><span style=color:#000;font-weight:700>=</span><span style=color:#099>10</span> leaves <span style=color:#099>1916040</span> word corpus <span style=color:#000;font-weight:700>(</span>98% of original 1944658, drops 28618<span style=color:#000;font-weight:700>)</span>
INFO : deleting the raw counts dictionary of <span style=color:#099>16385</span> items
INFO : <span style=color:teal>sample</span><span style=color:#000;font-weight:700>=</span>0.001 downsamples <span style=color:#099>47</span> most-common words
INFO : downsampling leaves estimated <span style=color:#099>1774595</span> word corpus <span style=color:#000;font-weight:700>(</span>92.6% of prior 1916040<span style=color:#000;font-weight:700>)</span>
INFO : estimated required memory <span style=color:#000;font-weight:700>for</span> <span style=color:#099>6442</span> words and <span style=color:#099>4000</span> dimensions: <span style=color:#099>209365000</span> bytes
INFO : resetting layer weights
INFO : training model with <span style=color:#099>3</span> workers on <span style=color:#099>6442</span> vocabulary and <span style=color:#099>4000</span> features, using <span style=color:teal>sg</span><span style=color:#000;font-weight:700>=</span><span style=color:#099>0</span> <span style=color:teal>hs</span><span style=color:#000;font-weight:700>=</span><span style=color:#099>0</span> <span style=color:teal>sample</span><span style=color:#000;font-weight:700>=</span>0.001 <span style=color:teal>negative</span><span style=color:#000;font-weight:700>=</span><span style=color:#099>5</span> <span style=color:teal>window</span><span style=color:#000;font-weight:700>=</span><span style=color:#099>5</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 2.38% examples, <span style=color:#099>35588</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 4.91% examples, <span style=color:#099>38060</span> words/s, in_qsize 6, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 8.99% examples, <span style=color:#099>42107</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 12.34% examples, <span style=color:#099>44613</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 15.95% examples, <span style=color:#099>45907</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 19.06% examples, <span style=color:#099>45430</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 22.46% examples, <span style=color:#099>45774</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 25.17% examples, <span style=color:#099>45869</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 28.19% examples, <span style=color:#099>46279</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 31.21% examples, <span style=color:#099>46555</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 34.39% examples, <span style=color:#099>46034</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 37.29% examples, <span style=color:#099>46612</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 39.90% examples, <span style=color:#099>46921</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 42.32% examples, <span style=color:#099>46297</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 45.59% examples, <span style=color:#099>46329</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 50.48% examples, <span style=color:#099>46561</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 54.17% examples, <span style=color:#099>46626</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 57.08% examples, <span style=color:#099>46427</span> words/s, in_qsize 4, out_qsize <span style=color:#099>1</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 60.25% examples, <span style=color:#099>46679</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 63.78% examples, <span style=color:#099>46823</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 65.84% examples, <span style=color:#099>46732</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 68.77% examples, <span style=color:#099>46813</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 71.74% examples, <span style=color:#099>46913</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 74.46% examples, <span style=color:#099>46662</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 77.82% examples, <span style=color:#099>46772</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 80.54% examples, <span style=color:#099>46766</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 83.49% examples, <span style=color:#099>46776</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 86.49% examples, <span style=color:#099>46701</span> words/s, in_qsize 6, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 89.55% examples, <span style=color:#099>46858</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 92.42% examples, <span style=color:#099>46847</span> words/s, in_qsize 5, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 94.90% examples, <span style=color:#099>46808</span> words/s, in_qsize 6, out_qsize <span style=color:#099>0</span>
INFO : EPOCH <span style=color:#099>1</span> - PROGRESS: at 98.11% examples, <span style=color:#099>46949</span> words/s, in_qsize 4, out_qsize <span style=color:#099>0</span>
INFO : worker thread finished; awaiting finish of <span style=color:#099>2</span> more threads
INFO : worker thread finished; awaiting finish of <span style=color:#099>1</span> more threads
INFO : worker thread finished; awaiting finish of <span style=color:#099>0</span> more threads
</code></pre></div><p>Let&rsquo;s see our vocab size</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic>#length of vocab</span>
<span style=color:#0086b3>len</span>(word_model<span style=color:#000;font-weight:700>.</span>wv<span style=color:#000;font-weight:700>.</span>vocab)
</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#099>6642</span>
</code></pre></div><p>Now that model is trained I would save the model so that I can resume it from here and I don&rsquo;t have to do this all over again.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#998;font-style:italic># Save the model </span>
word_model<span style=color:#000;font-weight:700>.</span>save(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>english</span><span style=color:#d14>&#39;</span>)
</code></pre></div><p>which will inform you of all the various things it saves</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>INFO : saving Word2Vec object under english, separately None
INFO : storing np array <span style=color:#d14>&#39;vectors&#39;</span> to english.wv.vectors.npy
INFO : not storing attribute vectors_norm
INFO : storing np array <span style=color:#d14>&#39;syn1neg&#39;</span> to english.trainables.syn1neg.npy
INFO : saved english
</code></pre></div><h2 id=visualizing-the-word-embedding-in-tensorboard>Visualizing the word embedding in Tensorboard</h2><p>+++</p><p>Tensorboard is a visualizing tool that is used with <a href=https://www.tensorflow.org/>tensorflow</a> to visualize certain parameters on the go to see how well the model is performing or visualize word embedding in 3D space.</p><p>The problem here is that the model I saved and the is not what tensorboard is familiar to work with so I had to convert this word vectors to a format that tensorboard understands.</p><p>This is a little difficult but there is a script written by <a href=https://gist.github.com/BrikerMan/7bd4e4bd0a00ac9076986148afc06507>BrikerMan</a> which is open source and it does the thing I want.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#000;font-weight:700>import</span> <span style=color:#555>sys</span><span style=color:#000;font-weight:700>,</span> <span style=color:#555>os</span>
<span style=color:#000;font-weight:700>from</span> <span style=color:#555>gensim.models</span> <span style=color:#000;font-weight:700>import</span> Word2Vec
<span style=color:#000;font-weight:700>import</span> <span style=color:#555>tensorflow</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>tf</span>
<span style=color:#000;font-weight:700>import</span> <span style=color:#555>numpy</span> <span style=color:#000;font-weight:700>as</span> <span style=color:#555>np</span>
<span style=color:#000;font-weight:700>from</span> <span style=color:#555>tensorflow.contrib.tensorboard.plugins</span> <span style=color:#000;font-weight:700>import</span> projector

<span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>visualize</span>(model, output_path):
    meta_file <span style=color:#000;font-weight:700>=</span> <span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>w2x_metadata.tsv</span><span style=color:#d14>&#34;</span>
    placeholder <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((<span style=color:#0086b3>len</span>(model<span style=color:#000;font-weight:700>.</span>wv<span style=color:#000;font-weight:700>.</span>index2word), <span style=color:#099>4000</span>))

    <span style=color:#000;font-weight:700>with</span> <span style=color:#0086b3>open</span>(os<span style=color:#000;font-weight:700>.</span>path<span style=color:#000;font-weight:700>.</span>join(output_path,meta_file), <span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>wb</span><span style=color:#d14>&#39;</span>) <span style=color:#000;font-weight:700>as</span> file_metadata:
        <span style=color:#000;font-weight:700>for</span> i, word <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>enumerate</span>(model<span style=color:#000;font-weight:700>.</span>wv<span style=color:#000;font-weight:700>.</span>index2word):
            placeholder[i] <span style=color:#000;font-weight:700>=</span> model[word]
            <span style=color:#998;font-style:italic># temporary solution for https://github.com/tensorflow/tensorflow/issues/9094</span>
            <span style=color:#000;font-weight:700>if</span> word <span style=color:#000;font-weight:700>==</span> <span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>&#39;</span>:
                <span style=color:#000;font-weight:700>print</span>(<span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard</span><span style=color:#d14>&#34;</span>)
                file_metadata<span style=color:#000;font-weight:700>.</span>write(<span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>{0}</span><span style=color:#d14>&#34;</span><span style=color:#000;font-weight:700>.</span>format(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>&lt;Empty Line&gt;</span><span style=color:#d14>&#39;</span>)<span style=color:#000;font-weight:700>.</span>encode(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>utf-8</span><span style=color:#d14>&#39;</span>) <span style=color:#000;font-weight:700>+</span> <span style=color:#d14>b</span><span style=color:#d14>&#39;</span><span style=color:#d14>\n</span><span style=color:#d14>&#39;</span>)
            <span style=color:#000;font-weight:700>else</span>:
                file_metadata<span style=color:#000;font-weight:700>.</span>write(<span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>{0}</span><span style=color:#d14>&#34;</span><span style=color:#000;font-weight:700>.</span>format(word)<span style=color:#000;font-weight:700>.</span>encode(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>utf-8</span><span style=color:#d14>&#39;</span>) <span style=color:#000;font-weight:700>+</span> <span style=color:#d14>b</span><span style=color:#d14>&#39;</span><span style=color:#d14>\n</span><span style=color:#d14>&#39;</span>)

    <span style=color:#998;font-style:italic># define the model without training</span>
    sess <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>InteractiveSession()

    embedding <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>Variable(placeholder, trainable <span style=color:#000;font-weight:700>=</span> <span style=color:#999>False</span>, name <span style=color:#000;font-weight:700>=</span> <span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>w2x_metadata</span><span style=color:#d14>&#39;</span>)
    tf<span style=color:#000;font-weight:700>.</span>global_variables_initializer()<span style=color:#000;font-weight:700>.</span>run()

    saver <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>train<span style=color:#000;font-weight:700>.</span>Saver()
    writer <span style=color:#000;font-weight:700>=</span> tf<span style=color:#000;font-weight:700>.</span>summary<span style=color:#000;font-weight:700>.</span>FileWriter(output_path, sess<span style=color:#000;font-weight:700>.</span>graph)

    <span style=color:#998;font-style:italic># adding into projector</span>
    config <span style=color:#000;font-weight:700>=</span> projector<span style=color:#000;font-weight:700>.</span>ProjectorConfig()
    embed <span style=color:#000;font-weight:700>=</span> config<span style=color:#000;font-weight:700>.</span>embeddings<span style=color:#000;font-weight:700>.</span>add()
    embed<span style=color:#000;font-weight:700>.</span>tensor_name <span style=color:#000;font-weight:700>=</span> <span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>w2x_metadata</span><span style=color:#d14>&#39;</span>
    embed<span style=color:#000;font-weight:700>.</span>metadata_path <span style=color:#000;font-weight:700>=</span> meta_file

    <span style=color:#998;font-style:italic># Specify the width and height of a single thumbnail.</span>
    projector<span style=color:#000;font-weight:700>.</span>visualize_embeddings(writer, config)
    saver<span style=color:#000;font-weight:700>.</span>save(sess, os<span style=color:#000;font-weight:700>.</span>path<span style=color:#000;font-weight:700>.</span>join(output_path,<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>w2x_metadata.ckpt</span><span style=color:#d14>&#39;</span>))
    <span style=color:#000;font-weight:700>print</span>(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>Run `tensorboard --logdir={0}` to run visualize result on tensorboard</span><span style=color:#d14>&#39;</span><span style=color:#000;font-weight:700>.</span>format(output_path))

<span style=color:#000;font-weight:700>if</span> __name__ <span style=color:#000;font-weight:700>==</span> <span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>__main__</span><span style=color:#d14>&#34;</span>:
    <span style=color:#d14></span><span style=color:#d14>&#34;&#34;&#34;</span><span style=color:#d14>
</span><span style=color:#d14></span><span style=color:#d14>    Just run `python w2v_visualizer.py word2vec.model visualize_result`</span><span style=color:#d14>
</span><span style=color:#d14></span><span style=color:#d14>    </span><span style=color:#d14>&#34;&#34;&#34;</span>
    <span style=color:#000;font-weight:700>try</span>:
        model_path <span style=color:#000;font-weight:700>=</span> sys<span style=color:#000;font-weight:700>.</span>argv[<span style=color:#099>1</span>]
        output_path  <span style=color:#000;font-weight:700>=</span> sys<span style=color:#000;font-weight:700>.</span>argv[<span style=color:#099>2</span>]
    <span style=color:#000;font-weight:700>except</span>:
        <span style=color:#000;font-weight:700>print</span>(<span style=color:#d14></span><span style=color:#d14>&#34;</span><span style=color:#d14>Please provice model path and output path</span><span style=color:#d14>&#34;</span>)
    model <span style=color:#000;font-weight:700>=</span> Word2Vec<span style=color:#000;font-weight:700>.</span>load(<span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>/home/jay/Saved_Models/english/english</span><span style=color:#d14>&#39;</span>)
visualize(model, <span style=color:#d14></span><span style=color:#d14>&#39;</span><span style=color:#d14>/home/jay/Saved_Models/english/</span><span style=color:#d14>&#39;</span>)
</code></pre></div><p>The script is very simple, what it does is, it takes the model I saved and makes a placeholder object of the size of my embeddings and starts writing tensorflow summary and checkpoints.</p><p>Well after a little tweaking I got it to work and the results is</p><p><img src=https://raw.githubusercontent.com/jdvala/website/master/img/tensorboard.png alt=img></p></div></article><button class=floating-button>
<a class=floating-button__link href=https://jdvala.github.io><span>home</span></a></button></div><footer class=post-footer><div class=footer><div>© 2020, Jay Vala. Theme - Origin by Andrey Parfenov</div><div class=footer__socials><a href=www.github.com/jdvala target=_blank class=social-link title="Github link" rel=noopener aria-label="follow on Github——Opens in a new window"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M0 0v24h24V0H0zm14.534 19.59c-.406.078-.534-.171-.534-.384v-2.195c0-.747-.262-1.233-.55-1.481 1.782-.198 3.654-.875 3.654-3.947.0-.874-.311-1.588-.824-2.147.083-.202.357-1.016-.079-2.117.0.0-.671-.215-2.198.82-.639-.18-1.323-.267-2.003-.271-.68.003-1.364.091-2.003.269-1.528-1.035-2.2-.82-2.2-.82-.434 1.102-.16 1.915-.077 2.118-.512.56-.824 1.273-.824 2.147.0 3.064 1.867 3.751 3.645 3.954-.229.2-.436.552-.508 1.07-.457.204-1.614.557-2.328-.666.0.0-.423-.768-1.227-.825.0.0-.78-.01-.055.487.0.0.525.246.889 1.17.0.0.463 1.428 2.688.944v1.489c0 .211-.129.459-.528.385-3.18-1.057-5.472-4.056-5.472-7.59.0-4.419 3.582-8 8-8s8 3.581 8 8c0 3.533-2.289 6.531-5.466 7.59z"/></svg></a></div></div></footer><script src=https://jdvala.github.io/js/script.js></script></body></html>