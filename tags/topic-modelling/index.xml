<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>topic-modelling on Jay Vala</title><link>https://jdvala.github.io/tags/topic-modelling/</link><description>Recent content in topic-modelling on Jay Vala</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2020, Jay Vala. Theme - Origin by Andrey Parfenov</copyright><lastBuildDate>Mon, 28 May 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://jdvala.github.io/tags/topic-modelling/index.xml" rel="self" type="application/rss+xml"/><item><title>Visulaizing LDA results</title><link>https://jdvala.github.io/posts/2018-05-28-lda-visualization/</link><pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-28-lda-visualization/</guid><description>This post will not work here so please click here
If you have trouble viewing it please contact me at: jay.vala@msn.com</description></item><item><title>Latent Dirichlet Allocation (Topic Modelling) - BaseLine</title><link>https://jdvala.github.io/posts/2018-05-27-latent-dirichlet-allocation-topic-modeling-baseline/</link><pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-27-latent-dirichlet-allocation-topic-modeling-baseline/</guid><description>Topic modeling can be useful when having a large corpus, when we want to unearth the meaning or of the data we have, which is too large to be done manually. In simple terms LDA is probabilistic unsupervised models that gives out top topics. So suppose we have a set of documents. we’ve chosen some fixed number of K topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic.</description></item></channel></rss>