<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>python on Jay Vala</title><link>https://jdvala.github.io/tags/python/</link><description>Recent content in python on Jay Vala</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2020, Jay Vala. Theme - Origin by Andrey Parfenov</copyright><lastBuildDate>Wed, 24 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://jdvala.github.io/tags/python/index.xml" rel="self" type="application/rss+xml"/><item><title>Pytest Environment Variables</title><link>https://jdvala.github.io/posts/pytest-environment-variables/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/pytest-environment-variables/</guid><description>How would you? ‚Ä¶ in my opinion üßê
Writing test for code can quickly go out of hand if there are a huge number of conditions in the code, which in general there is. I recently came across such a situation where my code was 200 lines and the tests I wrote for it were 400 lines üôà. I know it was too much but I have this practice where in I want to have 100% of test coverage.</description></item><item><title>Groupby and Apply with Dask on pandas DataFrame</title><link>https://jdvala.github.io/posts/2021-05-18-dask-groupby-apply/</link><pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2021-05-18-dask-groupby-apply/</guid><description>Recently, I have started using Dask to make some pandas computations faster, its a really good tool. It has a bit of learning curve, but its worth it.
Until now I used to use Pandarallel for parallelization of pandas apply functions. Its a good little tool and there are many alternatives to it as well. One of it is written by a good friend of my, and its called Mappy.</description></item><item><title>Create and Manage Python virtual environment</title><link>https://jdvala.github.io/posts/2021-04-25-easily-manage-and-create-virtual-environments/</link><pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2021-04-25-easily-manage-and-create-virtual-environments/</guid><description>In any fast paced AI startup, developers and data scientist have to manage a lot of repositories. For me personally, I manage 3 or sometimes 4 repositories at a time and switching between virtual environemts for these repositories is pain according to me, writing source deactivate to first deactivate the exisiting environment and then activating it by cding into the repo and source env/bin/activate.
To avoid this I wrote a function that will ease this pain.</description></item><item><title>Writing my own little interactive python environment</title><link>https://jdvala.github.io/posts/2021-02-15-my-own-ipython/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2021-02-15-my-own-ipython/</guid><description>Wouldn&amp;rsquo;t it be cool to build just for fun one of the most used and beloved tool in Data Science and Python community, the Interactive Python (ipython) shell.
The other day I was watching youtube videos, learning all about python and I came across this video by Sebastiaan Math√¥t, in which he had built his own ipython shell. I thought it was very cool and I wanted to replicate it. The link for the video is here</description></item><item><title>Walrus Operator - The new and shinny Python 3.8</title><link>https://jdvala.github.io/posts/2019-10-15-walrus-operator-python3.8/</link><pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2019-10-15-walrus-operator-python3.8/</guid><description>Yesterday we have welcomed python 3.8 with much fun fair. The whole change log can be found here but the highlight of the new python is the Walrus Operator.
According to the official documentation := will be the new walrus operator. This syntax assigns values to variables as a part of larger expression
Consider the following code snippet
if len(a) &amp;gt; 10: n = len(a) print(f&amp;#34;List is too long expected len &amp;lt; 10 but found {n}&amp;#34;) In the above expression len(a) is redundent but we can not help but write it twice, but not in python 3.</description></item><item><title>NLP and ML notes</title><link>https://jdvala.github.io/posts/2018-11-02-basic-notes/</link><pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-11-02-basic-notes/</guid><description>Notes: Natural Language Processing and Machine Learning Question: What are word embedding or what are word vectors?
Answer: Word embedding are learned representation for text where words that have the same meaning have a similar representation. Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.</description></item><item><title>NLP and ML notes - Part 2</title><link>https://jdvala.github.io/posts/2018-11-05-basic-notes-2/</link><pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-11-05-basic-notes-2/</guid><description>Bayes Theorem In conditional probability we have a order of probabilities. That is
$$P(A|B) =\frac{P(A\cap B)}{P(B)}$$
But it may not happen that we have the probability in this order or may happen that we may not have the probability of $P(B)$, thats when Bayes Theorm comes into play.
Bayes Theorem lets you swap the order of dependence between events. So we can calculate $P(A|B)$ in terms of $P(B|A)$. This is useful as mentioned above when it is difficult to calculate any one of the term in conditinal probability.</description></item><item><title>NLP and ML notes - Part 2</title><link>https://jdvala.github.io/posts/2018-11-06-basic-notes-3/</link><pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-11-06-basic-notes-3/</guid><description>Notes 3 Question: What is Stochastic Process?
Answer: A Stochastic Process is one that generates numbers with certain probability distribution (The word Stochastic simply means probablistic or randomly generated, but it is commonly used in case when referring to a sequence of results assumed to be generated by some underlying probability distribution.)
Question: What is Bernoulli trail?
Answer: A random expriment with exactly two results, success or failure.
Question: What is a Probabilty Mass Function?</description></item><item><title>Tensorboard</title><link>https://jdvala.github.io/posts/2018-10-23-visualizing-model-using-tensorboard/</link><pubDate>Tue, 23 Oct 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-10-23-visualizing-model-using-tensorboard/</guid><description>In this assignment we are going to use Tensorboard a tool provided with tensorflow to visualize and debug(if necessary) our neural networks visually. I am going to visualize all the parameteres that have the properties which enables us to debug our neural networks or gives us a fair intution on how and what goes wrong.
I will try and do this assignment in google colab, although it will be difficult, I will try my best.</description></item><item><title>Multilayer Perceptron</title><link>https://jdvala.github.io/posts/2018-10-13-mnist-multilayer-perceptron/</link><pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-10-13-mnist-multilayer-perceptron/</guid><description>Introduction to Deep Learning Assignment 1: Multilayer Perceptron MNIST Dataset NOTE: This Tutorial was done on google colab so it includes some google cloud helper functions.
In this assignment we are going to build a very basic deep learning model called as multilayer perceptron. Also we are going to expriment on it a little bit as suggested on the course website.
Multilayer Perceptron Multilayer perceptron is a type of feed forward network, it has minimum 3 layers(although we don&amp;rsquo;t count input and output as layers) input layer, hidden layer and output layer.</description></item><item><title>Finding K in K-Means</title><link>https://jdvala.github.io/posts/2018-07-09-k-means-clusturing/</link><pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-07-09-k-means-clusturing/</guid><description>Finding K in K-Means So when applying K-Means Clustring, one comes at a point where he/she has to decide how many cluster they want, now one can not go and tell 2,3, or 4 clusters there should be a some evidence that clustering the data into &amp;lsquo;k&amp;rsquo; clusters will yeild good results. So I met this problem, after searching for what can be done about this problem I stumble upon a something called &amp;lsquo;Silhouette Analysis&amp;rsquo;</description></item><item><title>Visulaizing LDA results</title><link>https://jdvala.github.io/posts/2018-05-28-lda-visualization/</link><pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-28-lda-visualization/</guid><description>This post will not work here so please click here
If you have trouble viewing it please contact me at: jay.vala@msn.com</description></item><item><title>Latent Dirichlet Allocation (Topic Modelling) - BaseLine</title><link>https://jdvala.github.io/posts/2018-05-27-latent-dirichlet-allocation-topic-modeling-baseline/</link><pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-27-latent-dirichlet-allocation-topic-modeling-baseline/</guid><description>Topic modeling can be useful when having a large corpus, when we want to unearth the meaning or of the data we have, which is too large to be done manually. In simple terms LDA is probabilistic unsupervised models that gives out top topics. So suppose we have a set of documents. we‚Äôve chosen some fixed number of K topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic.</description></item><item><title>LSTM with 8 Classes</title><link>https://jdvala.github.io/posts/2018-05-25-rnn-sequence-classification-with-modifications/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-25-rnn-sequence-classification-with-modifications/</guid><description>New LSTM with 8 classes In this script I will be using only 8 out of 32 classes that were originaly present in the dataset, this is necessary because the data in other classes is much less compared to these 8 classes, this makes it difficult for the neural network to learn anything off of those classes.
import pickle from gensim.models import Word2Vec import keras import numpy as np import pandas as pd from keras.</description></item><item><title>Curating the Dataset again!</title><link>https://jdvala.github.io/posts/2018-05-23-creating-data-set-again-/</link><pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-23-creating-data-set-again-/</guid><description>After the initial results it was clear that I have a lot less data then what the neural networks need to perform better. So I have decided that I will use everything in the files as my training data instead of removing everything except SUMMARY of the document. Also I will remove all the line with all the captial letters in order to remove the tags.
import os import sys import re import string from string import punctuation from nltk.</description></item><item><title>Quantitative Analysis of Dataset</title><link>https://jdvala.github.io/posts/2018-05-22-dataset-quantitative-analysis/</link><pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-22-dataset-quantitative-analysis/</guid><description>I have got bad results after training the RNN. Hence I have decided to analyse the data set and see the distribution of samples across different classes. This may give me some idea to why my network is performing poor.
import pandas as pd from collections import Counter import numpy as np data = pd.read_csv(&amp;#39;/home/jay/Data_Set_Creation/Data_to_Use.csv&amp;#39;) data.columns Index(['Sentence', 'Label', 'Numerical_Label'], dtype='object') labels = data.Numerical_Label.tolist() After the labels are into the list, I will count the number of samples in each class to see what is the distribution.</description></item><item><title>(UPDATED)Sequence Classification using Recurrent Neural Networks</title><link>https://jdvala.github.io/posts/2018-05-18-rnn_english/</link><pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-18-rnn_english/</guid><description>Update: Please go to the end of the post to see the updated results.
Aim: The aim of this model is to classify sentences. I am going to use Recurrent Neural Network for this purpose, specifically LSTMs. LSTMs have an advantage compared to simple RNN cell or GRU cell. You can learn about LSTMs and RNNs, There is no point on going in detail here. May be I will write another post wherein I will describe RNNs and LSTMs more intutively, but for today these two links will suffice.</description></item><item><title>Removing Data Discrepancies</title><link>https://jdvala.github.io/posts/2018-05-17-removing-data-set-discrepancies/</link><pubDate>Thu, 17 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-17-removing-data-set-discrepancies/</guid><description>Removing Data Set Discrepancies There were a lot of discrepancies in the dataset I created in the last step and I have detailed it in the most recent post. The most efficent way of removing those long sentences, and making them suitable for learning was making it of a fixed length.
I have used a technique called Sliding Window Protocol here is the brief example of how this is used here for breaking long sentences into smaller ones of fixed size.</description></item><item><title>Data Discrepancies</title><link>https://jdvala.github.io/posts/2018-05-16-data-discrepancies/</link><pubDate>Wed, 16 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-16-data-discrepancies/</guid><description>Analysizing Discrepancies in Dataset a lot of discrepancies in the dataset created previously, I can to realize that when I was building the model. I always analyze the dataset before building the model because 90% of the time people don&amp;rsquo;t realize that their model is performing bad because their data was bad. I am not gonna let that happen.
So what all discrepancies I found?
The largest sequence was of 242 words.</description></item><item><title>English EurLex dataset creation</title><link>https://jdvala.github.io/posts/2018-05-15-data-set-creation/</link><pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-15-data-set-creation/</guid><description>In this notebook I will create the data set for english corpus. I will assign every senetence in the corpus with it corresponding label. So if the sentence is from Agriculture it will be assigned label decided for Agriculture and so on.
import os, sys import pandas as pd # First lets create list of topics label = [] for root, dirs, files in os.walk(&amp;#39;/home/jay/GITHUB/Data_Thesis/Ready/&amp;#39;): label.append(dirs) label[0] ['transport', 'audiovisual_and_media', 'enlargement', 'internal_market', 'justice_freedom_security', 'institutional_affairs', 'economic_and_monetary_affairs', 'human_rights', 'agriculture', 'enterprise', 'food_safety', 'consumers', 'competition', 'humanitarian_aid', 'maritime_affairs_and_fisheries', 'environment', 'regional_policy', 'external_trade', 'fight_against_fraud', 'research_innovation', 'development', 'external_relations', 'foreign_and_security_policy', 'culture', 'employment_and_social_policy', 'energy', 'education_training_youth', 'customs', 'taxation', 'budget', 'public_health', 'information_society'] len(label[0]) 32 # Make a dictionary to hold the labels and corresponding category labels = dict(list(enumerate(label[0]))) labels {0: 'transport', 1: 'audiovisual_and_media', 2: 'enlargement', 3: 'internal_market', 4: 'justice_freedom_security', 5: 'institutional_affairs', 6: 'economic_and_monetary_affairs', 7: 'human_rights', 8: 'agriculture', 9: 'enterprise', 10: 'food_safety', 11: 'consumers', 12: 'competition', 13: 'humanitarian_aid', 14: 'maritime_affairs_and_fisheries', 15: 'environment', 16: 'regional_policy', 17: 'external_trade', 18: 'fight_against_fraud', 19: 'research_innovation', 20: 'development', 21: 'external_relations', 22: 'foreign_and_security_policy', 23: 'culture', 24: 'employment_and_social_policy', 25: 'energy', 26: 'education_training_youth', 27: 'customs', 28: 'taxation', 29: 'budget', 30: 'public_health', 31: 'information_society'} Now that we have the numbers we can create dataset and for training the RNN.</description></item><item><title>German Text Preprocessing</title><link>https://jdvala.github.io/posts/2018-05-11-german-preprocessing/</link><pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-11-german-preprocessing/</guid><description>Steps involved in preprocessing German Text German preprocessing of text is a little different than English text. There are special charaters with umlauts √§ that should be converted first into its native form(&amp;lsquo;√§&amp;rsquo; to &amp;lsquo;ae&amp;rsquo;), this is a charater level replacement so it will need time.
import os, re, sys from nltk.corpus import stopwords german_stop_words = stopwords.words(&amp;#39;german&amp;#39;) german_stop_words[len(german_stop_words)-6] 'w√ºrden' As we can see that even the stop words have those umlauts so we need to convert those too</description></item><item><title>Converting HTML to Text</title><link>https://jdvala.github.io/posts/2018-05-10-html-to-text/</link><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-10-html-to-text/</guid><description>I have recently downloaded all the html files for German corpus text, and it take a lot of time to download those files, I did that to see all the html tags which helps me to device a strategy to to parse the text files further, same as in the post, Parsing EU Summaries.
So the main idea here is read all the html text and convert it into respective text file to further preprocess it.</description></item><item><title>Word Embedding and It's visualization</title><link>https://jdvala.github.io/posts/2018-05-05-word-embedding-and-visulization/</link><pubDate>Sat, 05 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-05-word-embedding-and-visulization/</guid><description>Word Embeddings and It&amp;rsquo;s visualizations Creating Word Vectors +++ In the last post I have obtained perfect text out of the EU summaries, now the goal is to create word embeddings out of it and visualizing them in tensorboard projector. I am visualizing it right now because I want to know how to do it because I will need to visualize bilingual word embeddings when I create one to see how well the bilingual word embeddings are so that I can fine tune the process to fit best for my case.</description></item><item><title>English Corpus Preprocessing</title><link>https://jdvala.github.io/posts/2018-05-04-preprocessing-english-text/</link><pubDate>Fri, 04 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-04-preprocessing-english-text/</guid><description>Steps in preprocessing English text Before feeding text to any deep learning model it is advisable to preprocess the text to make it more suitable for the model. Preprocessing in general means cleaning the data to make it more convenient for any or all deep learning models to learn properly and efficiently.
Preprocessing is subjective, that means it depends on the purpose of preprocessing, depends on data, depends on language and also depends on the type of technique one would be adapting, but there are a lot of steps in preprocessing that does not depend on any of the above mentioned things, but its more of something that has to be done no matter what.</description></item><item><title>Parsing EU Summaries</title><link>https://jdvala.github.io/posts/parsing-eur-lex-summaries/</link><pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/parsing-eur-lex-summaries/</guid><description>Notes about findings in latest parsed documents Things to look out for when parsing the data WHEN DOES THE DIRECTIVE ENTER INTO FORCE? WHEN DOES THE STRATEGY APPLY? WHEN DO THE RULES OF PROCEDURE APPLY? WHEN DOES THE CONVENTION APPLY? WHEN DOES THE FRAMEWORK DECISION APPLY? WHEN DO THE REGULATION AND DECISION APPLY? WHEN DOES THE AGREEMENT AND PROTOCOL APPLY DATE OF ENTRY INTO FORCE WHEN DOES THE AGREEMENT APPLY WHEN DOES THIS ACT APPLY WHEN DO THE CONVENTION AND THE DECISION APPLY WHEN DOES THE COMMUNICATION APPLY WHEN DOES THE DECISION APPLY WHEN DO THE DECISIONS APPLY WHEN DO THE DECISION AND THE CONVENTION APPLY WHEN DO THE DECISION AND THE PROTOCOL APPLY WHEN DOES THE DIRECTIVE APPLY WHEN DOES THIS DIRECTIVE APPLY WHEN DOES THE PARTNERSHIP AGREEMENT APPLY WHEN DOES THE RECOMMENDATION APPLY WHEN DOES THE REGULATION APPLY WHEN DOES THIS REGULATION APPLY WHEN DO THE REGULATIONS APPLY WHEN DOES THIS REGULATION APPLY WHEN DO THE RULES APPLY?</description></item></channel></rss>