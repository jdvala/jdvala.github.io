<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>bayes on Jay Vala</title><link>https://jdvala.github.io/tags/bayes/</link><description>Recent content in bayes on Jay Vala</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2020, Jay Vala. Theme - Origin by Andrey Parfenov</copyright><lastBuildDate>Fri, 02 Nov 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://jdvala.github.io/tags/bayes/index.xml" rel="self" type="application/rss+xml"/><item><title>NLP and ML notes - Part 2</title><link>https://jdvala.github.io/posts/2018-11-05-basic-notes-2/</link><pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-11-05-basic-notes-2/</guid><description>Bayes Theorem In conditional probability we have a order of probabilities. That is
$$P(A|B) =\frac{P(A\cap B)}{P(B)}$$
But it may not happen that we have the probability in this order or may happen that we may not have the probability of $P(B)$, thats when Bayes Theorm comes into play.
Bayes Theorem lets you swap the order of dependence between events. So we can calculate $P(A|B)$ in terms of $P(B|A)$. This is useful as mentioned above when it is difficult to calculate any one of the term in conditinal probability.</description></item></channel></rss>