<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>analysis on Jay Vala</title><link>https://jdvala.github.io/tags/analysis/</link><description>Recent content in analysis on Jay Vala</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2020, Jay Vala. Theme - Origin by Andrey Parfenov</copyright><lastBuildDate>Fri, 02 Nov 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://jdvala.github.io/tags/analysis/index.xml" rel="self" type="application/rss+xml"/><item><title>NLP and ML notes</title><link>https://jdvala.github.io/posts/2018-11-02-basic-notes/</link><pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-11-02-basic-notes/</guid><description>Notes: Natural Language Processing and Machine Learning Question: What are word embedding or what are word vectors?
Answer: Word embedding are learned representation for text where words that have the same meaning have a similar representation. Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.</description></item><item><title>NLP and ML notes - Part 2</title><link>https://jdvala.github.io/posts/2018-11-05-basic-notes-2/</link><pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-11-05-basic-notes-2/</guid><description>Bayes Theorem In conditional probability we have a order of probabilities. That is
$$P(A|B) =\frac{P(A\cap B)}{P(B)}$$
But it may not happen that we have the probability in this order or may happen that we may not have the probability of $P(B)$, thats when Bayes Theorm comes into play.
Bayes Theorem lets you swap the order of dependence between events. So we can calculate $P(A|B)$ in terms of $P(B|A)$. This is useful as mentioned above when it is difficult to calculate any one of the term in conditinal probability.</description></item><item><title>NLP and ML notes - Part 2</title><link>https://jdvala.github.io/posts/2018-11-06-basic-notes-3/</link><pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-11-06-basic-notes-3/</guid><description>Notes 3 Question: What is Stochastic Process?
Answer: A Stochastic Process is one that generates numbers with certain probability distribution (The word Stochastic simply means probablistic or randomly generated, but it is commonly used in case when referring to a sequence of results assumed to be generated by some underlying probability distribution.)
Question: What is Bernoulli trail?
Answer: A random expriment with exactly two results, success or failure.
Question: What is a Probabilty Mass Function?</description></item><item><title>Tensorboard</title><link>https://jdvala.github.io/posts/2018-10-23-visualizing-model-using-tensorboard/</link><pubDate>Tue, 23 Oct 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-10-23-visualizing-model-using-tensorboard/</guid><description>In this assignment we are going to use Tensorboard a tool provided with tensorflow to visualize and debug(if necessary) our neural networks visually. I am going to visualize all the parameteres that have the properties which enables us to debug our neural networks or gives us a fair intution on how and what goes wrong.
I will try and do this assignment in google colab, although it will be difficult, I will try my best.</description></item><item><title>Multilayer Perceptron</title><link>https://jdvala.github.io/posts/2018-10-13-mnist-multilayer-perceptron/</link><pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-10-13-mnist-multilayer-perceptron/</guid><description>Introduction to Deep Learning Assignment 1: Multilayer Perceptron MNIST Dataset NOTE: This Tutorial was done on google colab so it includes some google cloud helper functions.
In this assignment we are going to build a very basic deep learning model called as multilayer perceptron. Also we are going to expriment on it a little bit as suggested on the course website.
Multilayer Perceptron Multilayer perceptron is a type of feed forward network, it has minimum 3 layers(although we don&amp;rsquo;t count input and output as layers) input layer, hidden layer and output layer.</description></item><item><title>Finding K in K-Means</title><link>https://jdvala.github.io/posts/2018-07-09-k-means-clusturing/</link><pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-07-09-k-means-clusturing/</guid><description>Finding K in K-Means So when applying K-Means Clustring, one comes at a point where he/she has to decide how many cluster they want, now one can not go and tell 2,3, or 4 clusters there should be a some evidence that clustering the data into &amp;lsquo;k&amp;rsquo; clusters will yeild good results. So I met this problem, after searching for what can be done about this problem I stumble upon a something called &amp;lsquo;Silhouette Analysis&amp;rsquo;</description></item><item><title>Visulaizing LDA results</title><link>https://jdvala.github.io/posts/2018-05-28-lda-visualization/</link><pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-28-lda-visualization/</guid><description>This post will not work here so please click here
If you have trouble viewing it please contact me at: jay.vala@msn.com</description></item><item><title>Latent Dirichlet Allocation (Topic Modelling) - BaseLine</title><link>https://jdvala.github.io/posts/2018-05-27-latent-dirichlet-allocation-topic-modeling-baseline/</link><pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-27-latent-dirichlet-allocation-topic-modeling-baseline/</guid><description>Topic modeling can be useful when having a large corpus, when we want to unearth the meaning or of the data we have, which is too large to be done manually. In simple terms LDA is probabilistic unsupervised models that gives out top topics. So suppose we have a set of documents. we’ve chosen some fixed number of K topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic.</description></item><item><title>Quantitative Analysis of Dataset</title><link>https://jdvala.github.io/posts/2018-05-22-dataset-quantitative-analysis/</link><pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-22-dataset-quantitative-analysis/</guid><description>I have got bad results after training the RNN. Hence I have decided to analyse the data set and see the distribution of samples across different classes. This may give me some idea to why my network is performing poor.
import pandas as pd from collections import Counter import numpy as np data = pd.read_csv(&amp;#39;/home/jay/Data_Set_Creation/Data_to_Use.csv&amp;#39;) data.columns Index(['Sentence', 'Label', 'Numerical_Label'], dtype='object') labels = data.Numerical_Label.tolist() After the labels are into the list, I will count the number of samples in each class to see what is the distribution.</description></item></channel></rss>