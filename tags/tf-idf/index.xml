<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TF-IDF on Jay Vala</title><link>https://jdvala.github.io/tags/tf-idf/</link><description>Recent content in TF-IDF on Jay Vala</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2020, Jay Vala. Theme - Origin by Andrey Parfenov</copyright><lastBuildDate>Fri, 02 Nov 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://jdvala.github.io/tags/tf-idf/index.xml" rel="self" type="application/rss+xml"/><item><title>NLP and ML notes</title><link>https://jdvala.github.io/posts/2018-11-02-basic-notes/</link><pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-11-02-basic-notes/</guid><description>Notes: Natural Language Processing and Machine Learning Question: What are word embedding or what are word vectors?
Answer: Word embedding are learned representation for text where words that have the same meaning have a similar representation. Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.</description></item></channel></rss>