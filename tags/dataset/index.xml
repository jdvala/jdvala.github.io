<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>dataset on Jay Vala</title><link>https://jdvala.github.io/tags/dataset/</link><description>Recent content in dataset on Jay Vala</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2020, Jay Vala. Theme - Origin by Andrey Parfenov</copyright><lastBuildDate>Mon, 09 Jul 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://jdvala.github.io/tags/dataset/index.xml" rel="self" type="application/rss+xml"/><item><title>Finding K in K-Means</title><link>https://jdvala.github.io/posts/2018-07-09-k-means-clusturing/</link><pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-07-09-k-means-clusturing/</guid><description>Finding K in K-Means So when applying K-Means Clustring, one comes at a point where he/she has to decide how many cluster they want, now one can not go and tell 2,3, or 4 clusters there should be a some evidence that clustering the data into &amp;lsquo;k&amp;rsquo; clusters will yeild good results. So I met this problem, after searching for what can be done about this problem I stumble upon a something called &amp;lsquo;Silhouette Analysis&amp;rsquo;</description></item><item><title>Visulaizing LDA results</title><link>https://jdvala.github.io/posts/2018-05-28-lda-visualization/</link><pubDate>Mon, 28 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-28-lda-visualization/</guid><description>This post will not work here so please click here
If you have trouble viewing it please contact me at: jay.vala@msn.com</description></item><item><title>Latent Dirichlet Allocation (Topic Modelling) - BaseLine</title><link>https://jdvala.github.io/posts/2018-05-27-latent-dirichlet-allocation-topic-modeling-baseline/</link><pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-27-latent-dirichlet-allocation-topic-modeling-baseline/</guid><description>Topic modeling can be useful when having a large corpus, when we want to unearth the meaning or of the data we have, which is too large to be done manually. In simple terms LDA is probabilistic unsupervised models that gives out top topics. So suppose we have a set of documents. we’ve chosen some fixed number of K topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic.</description></item><item><title>LSTM with 8 Classes</title><link>https://jdvala.github.io/posts/2018-05-25-rnn-sequence-classification-with-modifications/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-25-rnn-sequence-classification-with-modifications/</guid><description>New LSTM with 8 classes In this script I will be using only 8 out of 32 classes that were originaly present in the dataset, this is necessary because the data in other classes is much less compared to these 8 classes, this makes it difficult for the neural network to learn anything off of those classes.
import pickle from gensim.models import Word2Vec import keras import numpy as np import pandas as pd from keras.</description></item><item><title>Curating the Dataset again!</title><link>https://jdvala.github.io/posts/2018-05-23-creating-data-set-again-/</link><pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-23-creating-data-set-again-/</guid><description>After the initial results it was clear that I have a lot less data then what the neural networks need to perform better. So I have decided that I will use everything in the files as my training data instead of removing everything except SUMMARY of the document. Also I will remove all the line with all the captial letters in order to remove the tags.
import os import sys import re import string from string import punctuation from nltk.</description></item><item><title>Quantitative Analysis of Dataset</title><link>https://jdvala.github.io/posts/2018-05-22-dataset-quantitative-analysis/</link><pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-22-dataset-quantitative-analysis/</guid><description>I have got bad results after training the RNN. Hence I have decided to analyse the data set and see the distribution of samples across different classes. This may give me some idea to why my network is performing poor.
import pandas as pd from collections import Counter import numpy as np data = pd.read_csv(&amp;#39;/home/jay/Data_Set_Creation/Data_to_Use.csv&amp;#39;) data.columns Index(['Sentence', 'Label', 'Numerical_Label'], dtype='object') labels = data.Numerical_Label.tolist() After the labels are into the list, I will count the number of samples in each class to see what is the distribution.</description></item><item><title>(UPDATED)Sequence Classification using Recurrent Neural Networks</title><link>https://jdvala.github.io/posts/2018-05-18-rnn_english/</link><pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-18-rnn_english/</guid><description>Update: Please go to the end of the post to see the updated results.
Aim: The aim of this model is to classify sentences. I am going to use Recurrent Neural Network for this purpose, specifically LSTMs. LSTMs have an advantage compared to simple RNN cell or GRU cell. You can learn about LSTMs and RNNs, There is no point on going in detail here. May be I will write another post wherein I will describe RNNs and LSTMs more intutively, but for today these two links will suffice.</description></item><item><title>Removing Data Discrepancies</title><link>https://jdvala.github.io/posts/2018-05-17-removing-data-set-discrepancies/</link><pubDate>Thu, 17 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-17-removing-data-set-discrepancies/</guid><description>Removing Data Set Discrepancies There were a lot of discrepancies in the dataset I created in the last step and I have detailed it in the most recent post. The most efficent way of removing those long sentences, and making them suitable for learning was making it of a fixed length.
I have used a technique called Sliding Window Protocol here is the brief example of how this is used here for breaking long sentences into smaller ones of fixed size.</description></item><item><title>Data Discrepancies</title><link>https://jdvala.github.io/posts/2018-05-16-data-discrepancies/</link><pubDate>Wed, 16 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-16-data-discrepancies/</guid><description>Analysizing Discrepancies in Dataset a lot of discrepancies in the dataset created previously, I can to realize that when I was building the model. I always analyze the dataset before building the model because 90% of the time people don&amp;rsquo;t realize that their model is performing bad because their data was bad. I am not gonna let that happen.
So what all discrepancies I found?
The largest sequence was of 242 words.</description></item><item><title>English EurLex dataset creation</title><link>https://jdvala.github.io/posts/2018-05-15-data-set-creation/</link><pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate><guid>https://jdvala.github.io/posts/2018-05-15-data-set-creation/</guid><description>In this notebook I will create the data set for english corpus. I will assign every senetence in the corpus with it corresponding label. So if the sentence is from Agriculture it will be assigned label decided for Agriculture and so on.
import os, sys import pandas as pd # First lets create list of topics label = [] for root, dirs, files in os.walk(&amp;#39;/home/jay/GITHUB/Data_Thesis/Ready/&amp;#39;): label.append(dirs) label[0] ['transport', 'audiovisual_and_media', 'enlargement', 'internal_market', 'justice_freedom_security', 'institutional_affairs', 'economic_and_monetary_affairs', 'human_rights', 'agriculture', 'enterprise', 'food_safety', 'consumers', 'competition', 'humanitarian_aid', 'maritime_affairs_and_fisheries', 'environment', 'regional_policy', 'external_trade', 'fight_against_fraud', 'research_innovation', 'development', 'external_relations', 'foreign_and_security_policy', 'culture', 'employment_and_social_policy', 'energy', 'education_training_youth', 'customs', 'taxation', 'budget', 'public_health', 'information_society'] len(label[0]) 32 # Make a dictionary to hold the labels and corresponding category labels = dict(list(enumerate(label[0]))) labels {0: 'transport', 1: 'audiovisual_and_media', 2: 'enlargement', 3: 'internal_market', 4: 'justice_freedom_security', 5: 'institutional_affairs', 6: 'economic_and_monetary_affairs', 7: 'human_rights', 8: 'agriculture', 9: 'enterprise', 10: 'food_safety', 11: 'consumers', 12: 'competition', 13: 'humanitarian_aid', 14: 'maritime_affairs_and_fisheries', 15: 'environment', 16: 'regional_policy', 17: 'external_trade', 18: 'fight_against_fraud', 19: 'research_innovation', 20: 'development', 21: 'external_relations', 22: 'foreign_and_security_policy', 23: 'culture', 24: 'employment_and_social_policy', 25: 'energy', 26: 'education_training_youth', 27: 'customs', 28: 'taxation', 29: 'budget', 30: 'public_health', 31: 'information_society'} Now that we have the numbers we can create dataset and for training the RNN.</description></item></channel></rss>